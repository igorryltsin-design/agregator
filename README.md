# Agregator — локальный каталог научных материалов

Agregator — локальное веб-приложение для хранения, структурирования и интеллектуального поиска по научным материалам: статьям, диссертациям, презентациям, аудио и изображениям. Приложение индексирует выбранное хранилище, извлекает метаданные и текст, достраивает теги, строит граф связей и предоставляет расширенный интерфейс с поддержкой AI-поиска, статистики и прав доступа. Порт по умолчанию 5050.

## Содержание
- [Ключевые возможности](#ключевые-возможности)
- [Архитектура](#архитектура)
- [Поддерживаемые форматы и обработка контента](#поддерживаемые-форматы-и-обработка-контента)
- [Пользователи, роли и безопасность](#пользователи-роли-и-безопасность)
- [Веб-интерфейсы](#веб-интерфейсы)
- [Установка и запуск](#установка-и-запуск)
- [Конфигурация](#конфигурация)
- [Импорт, загрузка и сканирование](#импорт-загрузка-и-сканирование)
- [AI и интеллектуальные функции](#ai-и-интеллектуальные-функции)
- [Создание датасетов проблемных вопросов](#создание-датасетов-проблемных-вопросов)
- [Дообучение моделей на данных Agregator](#дообучение-моделей-на-данных-agregator)
- [Пайплайн проблемных вопросов из LLM Studio](#пайплайн-проблемных-вопросов-из-llm-studio)
- [REST API](#rest-api)
- [Сборка фронтендов](#сборка-фронтендов)
- [Дополнительная документация](#дополнительная-документация)
- [План доработок версии 2.0](#план-доработок-версии-20)
- [Структура репозитория](#структура-репозитория)
- [Резервное копирование и обслуживание](#резервное-копирование-и-обслуживание)
- [Лицензия](#лицензия)

## Changelog
- **v0.6.0** — Вынесены общие панели прогресса для AI-поиска и фона сканирования, добавлен компонент `ProgressPanel` с единой визуализацией.
- **v0.5.0** — Для иконок верхней панели добавлены унифицированные подсказки с кастомным стилем и доступностью.
- **v0.4.0** — Для страниц графа и статистики добавлены скелетоны загрузки и пустые состояния, чтобы интерфейс оставался информативным при отсутствии данных.
- **v0.3.0** — Добавлена модернизированная боковая панель фильтров с поиском по тегам и расширяемыми списками, управление вынесено в компонент `FilterSidebar`.
- **v0.2.0** — Переработан каталог: состояние вынесено в `useCatalogueState`, UI разбит на компоненты `AiPanel`, `FileCard`, `PreviewModal`, подготовлены вспомогательные хуки и обработчики для карточек.
- **v0.1.0** — Добавлена локальная санитизация AI-ответа с помощью DOMPurify.

## Ключевые возможности
- Полноценная многопользовательская модель с ролями `admin`/`user`, приватными коллекциями и ACL (viewer/editor/owner) для совместной работы.
- Интеллектуальное сканирование библиотеки: извлечение текста, OCR, автодетект типа материала, генерация тегов и метаданных с помощью LLM.
- Быстрый поиск: текстовый, фасетный, по тегам и годам, с морфологией и синонимами (Natasha + pymorphy2).
- AI-поиск с расширением запроса, реранжированием, глубоким просмотром фрагментов и генерацией краткого ответа со ссылками на источники.
- Визуализация графа связей «файл → теги» и дашборд статистики по типам, периодам, размерам и заполненности метаданных.
- Массовый импорт и выгрузка файлов, автосоздание коллекций, управление размещением и переименованием из интерфейса.
- Расширенная админ-панель: управление пользователями, LLM-эндпоинтами, очередью задач, логом действий, бэкапами.
- Экспорт набора в CSV/BibTeX, отдельный интерфейс AiWord для интеграции с внешними инструментами.
- Готовые Docker/Docker Compose конфигурации и вспомогательные скрипты (`start.sh`, `start.bat`).

## Архитектура
### Backend
- `Flask` + `SQLAlchemy` (см. `app.py`, `routes.py`, `models.py`). Приложение работает из коробки с SQLite (`catalogue.db`), поддерживает многопоточную обработку задач.
- REST API разделён между основным приложением (`app.py`) и блюпринтом `routes.py` (файлы, коллекции, экспорт, AI-подсистемы).
- Фоновое сканирование библиотеки, импорт и AI-поиск выполняются в потоках с прогрессом, фиксируются в таблицах `TaskRecord` и `UserActionLog`.
- Поддерживаются российская морфология (Natasha, pymorphy2), словарь синонимов, кеши текстовых фрагментов и миниатюр.
- Интеграция с внешними LLM через LM Studio / OpenAI-совместимые API, в том числе для реранжирования поиска, извлечения метаданных, суммаризации и анализа изображений.

### Frontend
- Основной интерфейс расположен в `frontend/` (React 18 + Vite, Bootstrap, Chart.js, vis-network). Продакшн-сборка попадает в `frontend/dist` и раздаётся Flask'ом.
- Дополнительный интерфейс AiWord (см. `AiWord/`) построен на Vite + Tailwind и публикуется под `/aiword`. Доступ управляется через модель `AiWordAccess`.

### Хранилище данных
- Основная библиотека файлов располагается в `SCAN_ROOT` (по умолчанию `sample_library/`). Пользовательские загрузки сохраняются в `library/` или выбранную коллекцию.
- База данных SQLite (`catalogue.db`) лежит в корне репозитория; резервные копии сохраняются в `backups/`.
- Кешированные тексты и миниатюры хранятся в `static/cache` и `static/thumbnails`; пересоздаются при полном сканировании.
- Текущие параметры работы UI сохраняются в `runtime_settings.json` и доступны через страницу настроек.

> Подробные схемы и диаграммы взаимодействий собраны в `docs/architecture.md`.

## Поддерживаемые форматы и обработка контента
- **Документы**: PDF (PyMuPDF), DOCX, RTF, TXT, EPUB, DJVU. Текстовые фрагменты ограничиваются по длине, сохраняются в базе и/или файловом кеше.
- **OCR**: для картинок и сканов используется `pytesseract`, языки и глубина сканирования настраиваются (`OCR_LANGS_CFG`, `PDF_OCR_PAGES_CFG`, `ALWAYS_OCR_FIRST_PAGE_DISSERTATION`).
- **Аудио**: WAV/MP3/FLAC и др. преобразуются через `ffmpeg` и расшифровываются `faster-whisper` (с загрузкой моделей через HuggingFace Hub).
- **Изображения**: извлекаются метаданные, миниатюры, при включённой опции `IMAGES_VISION_ENABLED` вызывается LLM для описания и ключевых слов.
- **Теги и метаданные**: автозаполнение из распознанного текста, словаря, LLM и из пользовательских схем (`TagSchema`).

## Пользователи, роли и безопасность
- Роли уровня приложения: `admin` (полный доступ), `editor` (редактирование в пределах доступных коллекций) и `viewer` (только просмотр). Историческая роль `user` автоматически трактуется как `editor`.
- Коллекции поддерживают приватный режим и ACL через `CollectionMember` с ролями `viewer`, `editor`, `owner`; администратор всегда имеет доступ.
- При первом запуске создаётся администратор `DEFAULT_ADMIN_USER` (по умолчанию `admin`). Пароль задаётся через `DEFAULT_ADMIN_PASSWORD` или автоматически генерируется из `ACCESS_CODE`/значения по умолчанию.
- Ограничение загрузок регулируется `MAX_CONTENT_LENGTH` (по умолчанию 50 МБ).
- Доступ к AiWord ограничивается таблицей `AiWordAccess`; права выдаются только администраторами.
- Для совместимости поддерживается опциональный «код доступа» (`ACCESS_CODE`) и basic-auth для legacy-интерфейсов.

## Веб-интерфейсы
- **Каталог (`/app`)**:
  - классический поиск с морфологией и синонимами, фильтры по типу, году, размеру, коллекции, тегам;
  - предпросмотр текста, изображений, PDF и аудио, отображение ключевых тегов и метаданных;
  - inline-редактор карточки, массовое применение тегов, переименование и перенос между коллекциями;
  - переключение в режим AI-поиска с отображением прогресса, ответом LLM и ссылками на найденные фрагменты.
  - встроенная кнопка «Справка» в верхней панели с описанием основных разделов, горячих клавиш и сценариев сканирования.
- **Граф (`/graph`)**: визуализация узлов файлов и тегов (автор, организация, консультант и др.), фильтрация по годам и ключам, smart-фильтр на основе морфологии.
- **Статистика (`/stats`)**: графики распределений по типу, месяцам, размерам, заполненности метаданных; экспорт данных в CSV.
- **Импорт и загрузка (`/upload`, `/import`)**: одиночные и массовые загрузки с автосозданием коллекций, фильтрацией по допустимым расширениям и автоприсвоением владельца.
- **Администрирование**:
  - настройки приложения (`/app/settings`): SCAN_ROOT, OCR, AI-возможности, директории по типам, параметры импорта;
  - управление пользователями, коллекциями и правами (`/app/admin/users`, `/api/admin/users`);
  - мониторинг задач (`/app/admin/tasks`), журнал действий, ручной запуск сканирования, выгрузка/очистка БД, управление LLM-эндпоинтами.
- **AiWord (`/aiword`)**: облегчённая витрина для интеграции с внешними инструментами, поддерживает поиск, фильтры и экспорт BibTeX. Требует отдельного допуска.

## Установка и запуск
### Зависимости
- Python 3.10+;
- SQLite (поставляется вместе с Python), Git;
- `tesseract-ocr` (для OCR), `ffmpeg`/`ffprobe` (для аудио);
- Опционально: Node.js 18+ (для пересборки фронтенда), LM Studio или другой OpenAI-совместимый сервер LLM.

### Быстрый старт (Python)
```bash
# Клонирование и окружение
python3 -m venv .venv
source .venv/bin/activate  # Windows PowerShell: .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

# Настройка окружения
cp .env.example .env   # задайте SCAN_ROOT и, при необходимости, LMSTUDIO_* и TRANSCRIBE_*

# (Опционально) сборка фронтенда
cd frontend && npm install && npm run build && cd ..

# Запуск приложения
python app.py  # UI будет доступен по адресу http://localhost:5050/app
```
При первом старте создаётся `catalogue.db` и базовая коллекция. Значение `SCAN_ROOT` можно менять из UI без перезапуска.

### Docker / Docker Compose
#### Базовый режим (`docker-compose.yml`)
```bash
docker compose build
docker compose up -d
```
Контейнер слушает порт `5050`, монтирует `./library` в `/data` и `./backups` в `/app/backups`. Переменные окружения вынесены в `docker/app.env`, дополнительные переопределения можно хранить в `docker-compose.override.yml`. Для ручной сборки образа см. `Dockerfile`. По умолчанию образ собирается для `linux/amd64`, поэтому на Apple Silicon (M1/M2) сразу получится получить совместимую x64-сборку; при необходимости измените поле `platform` в `docker-compose.yml`.

#### Перенос образа на другую машину
```bash
docker compose build  # соберёт linux/amd64 благодаря параметру platform
docker save agregator:local -o agregator.tar
# перенесите файл agregator.tar на целевую машину (scp/rsync/накопитель)
docker load -i agregator.tar
docker compose up -d
```

#### Локальная разработка (`docker-compose.dev.yml`)
```bash
# однократно
docker compose -f docker-compose.dev.yml build

# запуск с горячей перезагрузкой Flask
docker compose -f docker-compose.dev.yml up --remove-orphans
```
Файл использует `docker/app.dev.env` с безопасными dev-значениями, а код монтируется в контейнер. При изменении Python-файлов `flask run --reload` автоматически перезапускает приложение. Секреты и персональные ключи удобнее передавать через `docker-compose.override.yml` или переменные окружения.

#### Юнит-тесты (`docker-compose.test.yml`)
```bash
docker compose -f docker-compose.test.yml run --rm agregator-test
```
Команда запускает `pytest` в контейнере с профилем из `docker/app.test.env`; после завершения контейнер удаляется. Это полезно для CI/CD или проверки окружения в изолированном виде.

Подробное руководство по сценариям запуска приведено в `docs/deployment.md`.
### Скрипты запуска
- `start.sh` — пример запуска под `conda` с автоматической сборкой фронтенда.
- `start.bat` — аналогичный сценарий для Windows.

## Конфигурация
### Файл `.env`
| Переменная | Назначение |
| --- | --- |
| `SCAN_ROOT` | Абсолютный путь к директории с материалами. Можно изменить из UI. |
| `EXTRACT_TEXT` | Включить извлечение текста при сканировании (True/False). |
| `LMSTUDIO_API_BASE`, `LMSTUDIO_MODEL`, `LMSTUDIO_API_KEY` | Настройки подключения к LM Studio или OpenAI-совместимому API. |
| `LLM_CACHE_ENABLED`, `LLM_CACHE_TTL_SECONDS`, `LLM_CACHE_MAX_ITEMS`, `LLM_CACHE_ONLY_MODE` | Управление кэшем ответов LLM (вкл/выкл, TTL, размер, режим только чтения). |
| `SEARCH_CACHE_ENABLED`, `SEARCH_CACHE_TTL_SECONDS`, `SEARCH_CACHE_MAX_ITEMS` | Кэширование фасетов/результатов поиска (вкл/выкл, TTL, размер). |
| `DEFAULT_USE_LLM` | Включить LLM-обработку при сканировании по умолчанию. |
| `DEFAULT_PRUNE` | Удалять из базы записи об отсутствующих файлах по умолчанию. |
| `TRANSCRIBE_ENABLED`, `TRANSCRIBE_BACKEND`, `TRANSCRIBE_MODEL_PATH`, `TRANSCRIBE_LANGUAGE`, `FASTER_WHISPER_CACHE_DIR` | Настройки офлайн-транскрибации аудио. |
| `MAX_CONTENT_LENGTH` | Максимальный размер загружаемого файла (байт). |
| `AGREGATOR_TASK_WORKERS` | Количество потоков для внутренней очереди фоновых задач. |
| `AG_HTTP_TIMEOUT`, `AG_HTTP_CONNECT_TIMEOUT` | Таймауты (read/connect) для HTTP-клиента по умолчанию, сек. |
| `AG_HTTP_RETRIES`, `AG_HTTP_BACKOFF` | Число повторов и шаг backoff для повторов HTTP-запросов. |
| `LOG_LEVEL` | Глобальный уровень логирования приложения. |
| `SENTRY_DSN`, `SENTRY_ENVIRONMENT` | Параметры интеграции с Sentry (опционально). |
| `DEFAULT_ADMIN_USER`, `DEFAULT_ADMIN_PASSWORD` | Учетные данные первого администратора. |
| `ACCESS_CODE` | Опциональный код доступа в UI (legacy-режим). |
| Прочие (`OCR_LANGS`, `PDF_OCR_PAGES`, `OCR_DISS_FIRST_PAGE`) | Аргументы для OCR внутри Docker/приложения. |

Дополнительные профили для Docker Compose:
- `docker/app.dev.env` — дев-настройки с включённым debug и отключёнными тяжёлыми обработчиками.
- `docker/app.test.env` — минимальный профиль для прогонки тестов в контейнере.
### Файл `docker/app.env`
Содержит значения по умолчанию для контейнера. Перед запуском в продакшене пропишите `FLASK_SECRET_KEY` и другие чувствительные параметры.
Можно создать собственные варианты файлов и подключить их через `env_file` в override-компоуз файлах.

### Файл `docker/system-packages.txt`
Определяет набор системных пакетов Debian, устанавливаемых во время сборки образа. Добавляйте или удаляйте строки, если требуется изменить окружение OCR/аудио.

### Runtime-настройки (UI → Настройки)
Сохраняются в `runtime_settings.json`, в том числе:
- `SCAN_ROOT`, `IMPORT_SUBDIR`, `MOVE_ON_RENAME`, `TYPE_DIRS` — пути и политика размещения файлов;
- `EXTRACT_TEXT`, `PDF_OCR_PAGES_CFG`, `OCR_LANGS_CFG`, `ALWAYS_OCR_FIRST_PAGE_DISSERTATION` — параметры извлечения текста и OCR;
- `TRANSCRIBE_ENABLED`, `TRANSCRIBE_MODEL_PATH`, `AUDIO_KEYWORDS_LLM`, `SUMMARIZE_AUDIO` — обработка аудио;
- `IMAGES_VISION_ENABLED`, `TYPE_DETECT_FLOW`, `TYPE_LLM_OVERRIDE`, `KEYWORDS_TO_TAGS_ENABLED` — логика определения типа, тегов и автозаполнения;
- `AI_RERANK_LLM`, `LMSTUDIO_*` — использование LLM в поиске.
Настройки можно менять из UI; большинство применяются без перезапуска.

### LLM-эндпоинты
Администратор может добавить несколько эндпоинтов (`/api/admin/llm-endpoints`), указав модель, базовый URL и назначение (`rerank`, `summary`, `metadata`, `vision`, `keywords`, `default`). Внутри приложения работает round-robin и health-check по сигнатурам.

## Импорт, загрузка и сканирование
1. **Загрузка** — через `/upload` (одиночные файлы) или `/import` (папки). Можно создать новую коллекцию, назначить приватность, указать теги.
2. **Ручное копирование** — положите файлы в `SCAN_ROOT` или выбранный подкаталог (`IMPORT_SUBDIR`).
3. **Сканирование** — запустите из UI кнопку «Сканировать» или выполните `POST /scan/start` с флагами `extract_text`, `use_llm`, `prune`. Процесс:
   - проходит по файловой системе, актуализирует записи в БД;
   - извлекает текст/метаданные, выполняет OCR/LLM согласно настройкам;
   - создаёт и обновляет теги, очищает кеши при полном запуске;
   - при `prune=true` удаляет записи об отсутствующих файлах.
   Прогресс доступен по `/scan/status`, остановка — `POST /scan/cancel`.
4. **Перемещение и переименование** — доступно из UI; при изменении типа файла включённый `MOVE_ON_RENAME` переносит его в директорию `TYPE_DIRS[material_type]` (по умолчанию `dissertation`, `article`, `audio`, `image`, `other`).
5. **Освежение метаданных** — `POST /api/files/<id>/refresh` пересканирует конкретный файл, `/api/files/<id>/rename-suggest`/`rename` помогают с массовым переименованием.

## AI и интеллектуальные функции
- **Морфология и синонимы**: токенизация через Razdel, лемматизация pymorphy2, расширение запроса словарём `_RU_SYNONYMS`.
- **Поиск**: стандартный (`/api/search`, `/api/search_v2`) и AI-поиск (`/api/ai-search`, `/api/ai-search/stream`) с расширением ключевых слов, реранжированием и стриминговым прогрессом.
- **LLM-обработка**: извлечение метаданных, ключевых слов, типов материалов, генерация аннотаций и ответов. Поддерживаются кастомные промпты (`runtime_settings.json → PROMPTS`).
- **Аудио**: расшифровка faster-whisper, подсчёт длительности, генерация кратких резюме и ключевых слов (по флагам `SUMMARIZE_AUDIO`, `AUDIO_KEYWORDS_LLM`).
- **Изображения**: при включении vision-режима вызывается LLM для описания и тегов.
- **Экспорт в BibTeX**: эндпоинт `/api/aiword/bibtex` преобразует карточки в библиографические записи с учётом тегов.

## Создание датасетов проблемных вопросов
Agregator можно использовать как единый реестр кейсов, в которых модель дала некорректный ответ. Рекомендуемый рабочий цикл:

Шаг 1. Во время тестирования экспортируйте проблемные диалоги из LLM Studio (вкладка «История» → «Экспорт») или фиксируйте их вручную.

Шаг 2. Импортируйте выгрузку в Agregator в отдельную коллекцию, например `LLM Feedback`. При загрузке назначьте теги `llm-issue`, `regression`, `priority-*`, чтобы позже быстро отфильтровать записи.

Шаг 3. Для каждого материала заполните пользовательские метаданные: сохраните исходный вопрос в `metadata.llm.prompt`, ожидаемый ответ в `metadata.llm.reference`, полученный ответ в `metadata.llm.actual`, а также добавьте поле `metadata.llm.status` (`open`, `fixed`, `ready`). Эти поля доступны в карточке файла и через REST API.

Шаг 4. После проверки корректного эталона переведите `metadata.llm.status` в `ready` и добавьте тег `ready-for-training`. Используйте поиск и граф связей, чтобы исключить дубликаты и убедиться, что покрыты все типовые сценарии.

### Автоматизация через REST API
Agregator предоставляет REST-эндпоинты для загрузки и выгрузки таких кейсов. Пример создания записи напрямую из журнала автотестов:

```bash
curl -X POST http://localhost:5050/api/files \
  -H 'Content-Type: application/json' \
  -d '{
    "title": "LLM feedback: ошибка в расчёте тарифа",
    "text": "Вопрос: ...\nОтвет модели: ...",
    "collection": "LLM Feedback",
    "tags": ["llm-issue", "regression", "priority-high"],
    "metadata": {
      "llm": {
        "prompt": "Какая ставка применяется ...?",
        "actual": "Ставка 7%, потому что ...",
        "reference": "Для указанного региона ставка 5,5%",
        "status": "ready",
        "source": "test_suite_2024-10-12"
      }
    }
  }'
```

Выгрузку подготовленных примеров удобно делать из UI («Экспорт» → `CSV`) с фильтрами по коллекции и тегу `ready-for-training`. Для автоматизации используйте REST-эндпоинт `GET /export/csv` или `GET /api/files` с теми же фильтрами, что доступны в интерфейсе.

Полученный CSV или JSON имеет исходные метаданные. Пример преобразования выгрузки `llm-feedback.csv` в формат `dataset.jsonl`, совместимый с LLM Studio:

```python
import csv
import json

with open("llm-feedback.csv", newline="", encoding="utf-8") as src, open("dataset.jsonl", "w", encoding="utf-8") as dst:
    reader = csv.DictReader(src)
    for row in reader:
        llm_meta = json.loads(row.get("metadata") or "{}").get("llm", {})
        prompt = llm_meta.get("prompt") or row.get("title")
        reference = llm_meta.get("reference") or row.get("text")
        if not prompt or not reference:
            continue
        payload = {
            "input": prompt.strip(),
            "output": reference.strip(),
            "source": llm_meta.get("source") or row.get("path")
        }
        dst.write(json.dumps(payload, ensure_ascii=False) + "\n")
```

### Автоматизация из LLM Studio
Во вкладке «Обучение» LLM Studio появилась карточка «Agregator: авто сбор и обучение». Укажите URL, фильтры (коллекция, теги, статус) и при необходимости токен доступа — приложение самостоятельно:

- выгрузит подтверждённые кейсы через `/export/csv`;
- при желании добавит датасет из выбранного адаптера;
- запустит дообучение (метод, параметры и каталог вывода берутся из текущей конфигурации).

Задача создаётся на сервере `base_model_server.py` через новый эндпоинт `POST /v1/fine-tunes/from-agregator`. В ответе — ID и прогресс fine-tune, который сразу отображается в UI.

## Дообучение моделей на данных Agregator
После формирования `dataset.jsonl` выполните дообучение в LLM Studio:

Шаг 1. Откройте вкладку «Обучение» и нажмите «Импортировать датасет». Выберите `dataset.jsonl` или CSV из Agregator — парсер автоматически распознаёт формат и добавит пары вопросов-ответов.

Шаг 2. Проверьте статистику набора: количество примеров, максимальную длину и источник. При необходимости отфильтруйте или отредактируйте записи непосредственно в списке.

Шаг 3. Настройте параметры дообучения (метод, квантизация, скорость обучения) и запустите задачу. FineTuneManager сохранит снапшот датасета и метаданные обучения в каталоге `Models/<job-id>/`.

Шаг 4. После завершения адаптер станет доступен во вкладках «Чат» и «Обучение» через выпадающий список «Файнтюны».

### Повторное дообучение уже адаптированной модели
FineTuneManager сохраняет результаты каждой задачи в каталоге `Models/<job-id>/` вместе с `dataset.jsonl`. Чтобы расширить ранее обученную модель:

Шаг 1. Объедините старый и новый датасеты, например склеив `Models/<job-id>/dataset.jsonl` и свежую выгрузку из Agregator. Убедитесь, что формат `input`/`output` сохранён.

Шаг 2. В настройках LLM Studio укажите новый путь для вывода (`output_dir`) и запустите обучение с обновлённым датасетом. Базовую модель (`base_model_path`) можно оставить прежней.

Для LoRA/QLoRA адаптеров при необходимости можно стартовать новую задачу уже от дообученной модели. Слейте адаптер с базовой моделью и сохраните результат в отдельный каталог:

```python
from transformers import AutoModelForCausalLM
from peft import PeftConfig, PeftModel

adapter_dir = "Models/finetune-20250201-120305"
merged_dir = "Models/finetune-20250201-120305-merged"

config = PeftConfig.from_pretrained(adapter_dir)
base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(base_model, adapter_dir)
model = model.merge_and_unload()
model.save_pretrained(merged_dir)

# merged_dir используйте как base_model_path для следующего запуска fine-tune
```

Так можно последовательно добавлять новые примеры из Agregator, не теряя предыдущие улучшения модели.

## Пайплайн проблемных вопросов из LLM Studio
Для автоматизации цикла «тестирование → сбор данных → дообучение» добавлен эндпоинт `POST /api/training/problem-pipeline` (требует права администратора). Он принимает результаты вкладки «Тестирование» LLM Studio, извлекает проблемные вопросы, запрашивает релевантные материалы в Agregator и формирует датасет из абзацев:

```jsonc
POST /api/training/problem-pipeline
{
  "evaluation": {
    "items": [
      {
        "question": "Когда закаляется сталь?",
        "referenceAnswer": "При быстром охлаждении после нагрева выше точки А3",
        "modelAnswer": "При комнатной температуре",
        "score": 0.2
      }
    ],
    "quality_gate": 0.7
  },
  "target_pairs": 30,
  "generation": {
    "pairs_per_snippet": 1,
    "include_reference_pair": true
  },
  "search": {
    "top_k": 4,
    "deep_search": true
  },
  "fine_tune": {
    "server_url": "http://127.0.0.1:8001",
    "base_model_path": "Models/gemma",
    "config": {
      "method": "lora",
      "quantization": "none",
      "lora_rank": 16,
      "lora_alpha": 32,
      "learning_rate": 0.0002,
      "batch_size": 4,
      "epochs": 3,
      "max_length": 512,
      "warmup_steps": 0
    }
  }
}
```

Основные параметры:

- `evaluation.items` — результаты тестирования. Для отбора используются записи с `score` ниже порога `quality_gate` (или все, если порог не указан).
- `target_pairs` — целевое количество QA-пар в итоговом датасете.
- `generation` — настройки генерации вопросов: количество пар на сниппет, минимальная длина абзаца и т.п.
- `search` — параметры запроса в AI-поиске Agregator (`top_k`, `deep_search`, фильтры коллекций).
- `fine_tune` — конфигурация запроса к LLM Studio (`POST /v1/fine-tunes`). Если блок опущен или передан `"dry_run": true`, сервис вернёт только собранный датасет и логи.

В ответе возвращаются размер датасета, несколько примеров, диагностические логи, а при включённом `fine_tune` — описание созданной задачи дообучения. Для экспорта полного датасета используйте флаг `include_dataset` в запросе.

Для вызовов из внешних приложений установите переменную окружения `PIPELINE_API_KEY`. Тогда достаточно передать заголовок `Authorization: Bearer <token>` (или `X-Agregator-Key`) — и маршрут станет доступен без авторизационной cookie.

> В интерфейсе LLM Studio (вкладка «Тестирование») кнопка «Сформировать датасет» отправляет текущий прогон в этот эндпоинт. Параметры запроса (включая заголовок и токен) берутся из настроек автообучения, а результат отображается в журнале.

## REST API
Основные эндпоинты (требуют авторизации через сессию/куки):

| Endpoint | Метод(ы) | Описание |
| --- | --- | --- |
| `/api/auth/login`, `/api/auth/logout`, `/api/auth/me`, `/api/auth/password` | POST/GET | Аутентификация и смена пароля. |
| `/api/files` | GET, POST | Получение последних файлов, создание записи. |
| `/api/files/<id>` | GET, PUT | Детали файла, обновление метаданных/тегов. |
| `/api/files/<id>/refresh` | POST | Форсированное пересканирование файла. |
| `/api/files/<id>/rename(-suggest)` | GET/POST | Подсказка и применение шаблона переименования. |
| `/api/collections` | GET | Список доступных коллекций. |
| `/api/collections/<id>/members` | GET, POST, PATCH, DELETE | Управление участниками коллекции (ACL). |
| `/api/search`, `/api/search_v2` | GET | Стандартный поиск и фасетные данные. |
| `/api/facets` | GET | Фасеты по тегам и типам. |
| `/api/graph`, `/api/graph/build` | GET, POST | Построение графа связей и автодобавление тегов. |
| `/api/stats`, `/api/stats/tag-values` | GET | Дашборд статистики. |
| `/export/csv`, `/export/bibtex`, `/api/aiword/bibtex` | GET | Экспорт каталога. |
| `/api/collections/<id>/export/excel` | GET | Экспорт выбранной коллекции в Excel (гиперссылки на файлы). |
| `/api/ai-search`, `/api/ai-search/stream` | POST | AI-поиск (JSON/Server-Sent Events). |
| `/api/admin/users`, `/api/admin/tasks`, `/api/admin/llm-endpoints`, `/api/admin/actions` | GET/POST/PATCH/DELETE | Администрирование пользователей, задач, LLM, логов. |
| `/scan/start`, `/scan/status`, `/scan/cancel` | POST/GET | Управление фоновым сканированием. |
| `/admin/backup-db`, `/admin/clear-db`, `/admin/import-db` | POST | Обслуживание базы данных. |

Полный список маршрутов см. в `app.py` и `routes.py`.

## Сборка фронтендов
### Основной интерфейс (`frontend/`)
```bash
cd frontend
npm install
npm run dev    # дев-сервер Vite на 5173 (прокси можно настроить вручную)
npm run build  # продакшн-сборка в dist/
```
Сборка `dist/` автоматически подхватывается Flask'ом.

### AiWord (`AiWord/`)
```bash
cd AiWord
npm install
npm run build  # результат в AiWord/dist/
```
Готовая сборка нужна для корректного отображения `/aiword`. В Docker-образе ожидается наличие `AiWord/dist`.

### Экспорт автономной AiWord
Чтобы развернуть AiWord отдельно от остального приложения:
```bash
./scripts/export_aiword.sh          # копия в backups/aiword-standalone-<дата>
./scripts/export_aiword.sh ~/AiWord # пользовательский путь
./scripts/export_aiword.sh --zip    # дополнительный архив .tar.gz рядом с копией
```
В результирующей директории выполните `npm install` и запустите `npm run dev` или `npm run build && npm run preview` на целевой машине.

## Дополнительная документация
- `docs/architecture.md` — обзор архитектуры, потоков данных и диаграммы взаимодействий.
- `docs/deployment.md` — сценарии запуска (Docker, bare metal), типовые проблемы и чек-листы.
- `docs/ai_search_operations.md` и `docs/ai_search_roadmap.md` — эксплуатация AI-поиска, метрики и roadmap развития.
- `docs/facets_refactor.md` — записка по рефакторингу фасетов и подготовке фоновой индексации.

## План доработок версии 2.0

### Основные потоки
- **Платформа**
  1. Завершить переход на `create_app`, убрав глобальные зависимости и вынеся конфигурацию/инициализацию во фабрику.
  2. Разнести регистрацию blueprint'ов и сервисную логику по пакетам (`agregator/app_factory`, `agregator/services`, `routes`), оставив в `app.py` только точки входа.
- **Данные и БД**
  1. Перенести хранилище на PostgreSQL, подготовить миграции Alembic и сценарии перехода с SQLite.
  2. Формализовать схемы коллекций/тегов: ограничения целостности, индексы, каскады, политики удаления.
  3. Добавить аудит изменений метаданных (история изменений, экспорт, отчёты в админке).
- **Фоновые задачи**
  1. Заменить in-memory `TaskQueue` на брокер (Redis+RQ или Celery) с подтверждениями доставки и повторными попытками.
  2. Вынести OCR/Whisper/LLM в отдельные worker-сервисы с управляемыми лимитами и настройками конкурентности.
  3. Расширить админ-панель мониторингом очередей: глубина, состояние воркеров, время выполнения.
- **Поиск и AI**
  1. Реализовать roadmap улучшений: адаптивные стоп-листы, контроль сниппетов, сбор обратной связи пользователей.
  2. Обновить `SearchService`: вынести кандидатов в отдельный индекс, ввести стратегический кэш и профили глубокой проверки.
  3. Добавить лимиты LLM, SSE-стрим прогресса и расширенную телеметрию стадий поиска.
- **Фасеты и индексация**
  1. Завершить внедрение сервисного слоя фасетов с версионированным кэшем и управляемой инвалидацией.
  2. Настроить фоновые пересчёты агрегатов и расширяемые измерения (размер, период, пользовательские поля).
- **Фронтенд и UX**
  1. Обновить Shell (`App.tsx`) под новые статусы задач, профили поиска и темизацию.
  2. Добавить управление ключевыми словами, таймлайн прогресса и улучшенную доступность.

### Поддерживающие работы
- **Наблюдаемость**
  1. Стандартизировать логирование (JSON, структурные поля), унифицировать ротацию и хранение.
  2. Интегрировать Sentry/Prometheus, health-check'и и сбор ключевых метрик производительности.
  3. Подготовить runbook'и резервного копирования и очистки кэшей, закрепить процедуры в документации.
- **Безопасность и доступы**
  1. Расширить модель ролей/ACL для API, учесть сценарии `editor`/`viewer` и приватные коллекции.
  2. Добавить MFA/инвайты, контроль сессий и ограничения экспорта чувствительных данных.
  3. Завершить аудит ACL и логирование действий пользователей, автоматизировать отчётность.
- **Документация**
  1. Синхронизировать README и `docs/deployment.md` с изменениями v2, добавить гайд по миграции.
  2. Подготовить API reference и onboarding для новых администраторов/редакторов.
  3. Описать сценарии обслуживания, восстановления и плановых обновлений.

## Структура репозитория
- `app.py` — основной Flask-приложение, UI-маршруты, фоновые задачи, AI-поиск.
- `routes.py` — REST-блюпринт (файлы, коллекции, экспорт, AiWord-эндпоинты).
- `models.py` — SQLAlchemy-модели (пользователи, коллекции, файлы, теги, задачи, LLM-эндпоинты и др.).
- `frontend/` — React/Vite интерфейс каталога; `dist/` служит статикой.
- `AiWord/` — альтернативный UI и интеграция с внешними сервисами.
- `static/` — кеш текстов и миниатюр.
- `library/`, `sample_library/` — фактическое хранилище файлов (боевое и пример).
- `runtime_settings.json` — сохранённые параметры приложения.
- `docker-compose.yml`, `Dockerfile` — контейнеризация.
- `scripts/` — утилиты для проверки ffmpeg/faster-whisper и экспорт автономной AiWord.
- `README_DEPLOY.md` — дополнительные заметки по развёртыванию.

## Резервное копирование и обслуживание

### Мониторинг и здоровье
- `GET /health` — JSON-эндпоинт с проверкой БД, очереди задач и фонового шедулера очистки.
- `GET /metrics` — метрики в формате Prometheus (очередь задач и сводка по `TaskRecord`).
- `POST /admin/backup-db` сохраняет копию `catalogue.db` в `backups/` с меткой времени.
- `POST /admin/import-db` заменяет текущую базу загруженным файлом (перед заменой создаётся резервная копия).
- `POST /admin/clear-db` очищает таблицы файлов/тегов и удаляет кеши миниатюр/текстов.
- Очередь задач (`TaskRecord`) и лог действий (`UserActionLog`) доступны в админ-панели.
- Для обновления среза файлов используется плановое сканирование (`scan/start`); прогресс отображается в UI и через API.

## Лицензия
Проект распространяется по лицензии **MIT NonCommercial** (см. `LICENSE`): свободное использование в личных, образовательных и исследовательских целях; коммерческое использование требует согласования с правообладателем.
