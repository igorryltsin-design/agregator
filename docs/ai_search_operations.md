# AI Search Operations Guide

## Cache Storage
- Snippets generated by the LLM are cached in the `ai_search_snippet_cache` table.
- Cache key: `(file_id, query_hash, llm_variant)`; default TTL: `AI_SNIPPET_CACHE_TTL_HOURS` (24 hours).
- The cache is automatically refreshed on every hit; expired entries are lazily evicted.
- Фоновый планировщик (`AI_SNIPPET_CACHE_SWEEP_INTERVAL_HOURS`, по умолчанию 24 часа) запускает очистку автоматически; при необходимости скрипт `scripts/ai_cache_cleanup.py` можно вызвать вручную.
- To purge cache entries manually:
  ```sql
  DELETE FROM ai_search_snippet_cache;
  ```

## Feedback Loop
- Endpoint: `POST /api/ai-search/feedback`
  - Requires authentication.
  - Payload fields: `query_hash`, `action` (`click`, `relevant`, `irrelevant`, `ignored`), optional `file_id`, `keyword`, `score`.
- Collected feedback is stored in `ai_search_keyword_feedback` and used to suppress noisy keywords in subsequent searches.

## Metrics & Monitoring
- Endpoint: `GET /api/admin/ai-search/metrics?limit=100`
  - Returns recent search timings (`total_ms`, per-stage durations) and aggregated averages.
- Use the data as a datasource for Grafana/Loki dashboards.
- Recommended alerts:
  - `total_ms` > 20000 consistently ⇒ check LLM latency / deep search load.
  - `llm_snippet_ms` spikes ⇒ consider disabling `llm_snippets` or increasing cache TTL.
- Ручная очистка: `DELETE /api/admin/ai-search/metrics` — удаляет журнал метрик (кнопка доступна на странице «AI метрики»).

## Environment Variables
- `AI_SNIPPET_CACHE_TTL_HOURS` — snippet cache retention window (default: 24).
- `AI_SNIPPET_CACHE_SWEEP_INTERVAL_HOURS` — интервал фоновой очистки (default: 24).

## Maintenance Tasks
- Schedule a daily cleanup job (cron) to remove expired rows:
  ```sql
  DELETE FROM ai_search_snippet_cache WHERE expires_at IS NOT NULL AND expires_at < CURRENT_TIMESTAMP;
  ```
- Periodically review `ai_search_keyword_feedback` for abusive submissions; consider anonymising user IDs in exported analytics.

## RAG Utilities
- `scripts/rag_index.py`
  - `rebuild` — запускает полный цикл (ингест + эмбеддинги).
  - `ingest` — только разбивает документы на чанки (`--file-id` можно повторять).
  - `embed` — пересчитывает эмбеддинги для всех чанков.
  - `inspect --chunk <id>` — печатает содержимое и метаданные конкретного чанка.
- `scripts/rag_smoke.py`
  - Прогоняет набор запросов (по умолчанию 5 типовых) через `_ai_search_core` и выводит суммарное время, число найденных документов, наличие предупреждений RAG/фолбэка.
  - Пример: `python scripts/rag_smoke.py --deep --top-k 4 --json smoke.json` — сохраняет результат в JSON для последующего анализа.

## Monitoring Checklist
- Проверяйте `/api/admin/ai-search/metrics` — новые поля `rag_context_count`, `rag_fallback`, `rag_hallucination_warning` сигнализируют о качестве RAG-ответов.
- Журналируйте ежедневный запуск `scripts/rag_smoke.py` (в CI или cron) и отслеживайте аномалии по времени ответа.
- После обновления эмбеддингов используйте `scripts/rag_index.py inspect --chunk <id>` для выборочной проверки чанков.
- Включите алерты, если `rag_fallback` или предупреждения встречаются >10% запросов подряд — вероятны проблемы с индексом или текстами.
