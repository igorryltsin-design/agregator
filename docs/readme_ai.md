# Поиск и ИИ‑поиск в Agregator

Этот документ подробно описывает, как устроены обычный (классический) поиск и ИИ‑поиск: какие API используются, что и где запрашивается, как формируются кандидаты и сниппеты, как работает «полнотекстовый» этап, как собираются и выводятся строки прогресса, а также как флаги и фильтры влияют на результат.


## Кратко
- Классический поиск — быстрые SQL‑фильтры по полям и тегам, опционально «умный» режим с лемматизацией и синонимами для русского языка.
- ИИ‑поиск — многошаговый пайплайн: расширение запроса через LLM, токенизация и IDF‑веса, отбор кандидатов по тегам и метаданным (SQL), ранжирование с бустами, опционально «глубокий» проход по содержимому файлов (чтение текста, поиск совпадений и сбор сниппетов), краткий ответ LLM по фрагментам, опциональное LLM‑реранжирование.
- Прогресс — сервер собирает строки прогресса в процессе выполнения; потоковый API отсылает их по SSE. В фронтенде по умолчанию используется фолбэк без стриминга: строки прогресса «проигрываются» по таймеру после завершения поиска.


## Данные и модели
- База: `catalogue.db` (SQLite), ORM: SQLAlchemy.
- Таблицы и ключевые поля:
  - `files`: `title`, `author`, `year`, `material_type`, `keywords`, `abstract`, `text_excerpt`, `mtime`, `rel_path`, `size`, `sha1`.
  - `tags`: пары произвольных `key`/`value`, связанные с файлами (`file_id`).
  - `collections`: атрибуты коллекций; важны флаги `searchable`/`graphable`.
- Доступ: все запросы фильтруются по разрешённым коллекциям пользователя и по `collections.searchable = true`.
  - Фильтр доступа: `app.py:1013` (`_apply_file_access_filter`).


## Классический поиск
- API:
  - GET `/api/search` — «базовый» вариант.
  - GET `/api/search_v2` — расширенный: пагинация и дополнительные фильтры.
    - Код: `app.py:3239` и `app.py:3313`.

- Параметры запроса (оба API):
  - `q` — строка запроса.
  - `smart` — логический флаг «умного» режима (`1|true|yes|on`).
  - Фильтры: `type` (material_type), `tag=key=value` (многократно), `year_from`, `year_to`, `size_min`, `size_max`, `collection_id`.
  - Для `/api/search_v2`: `limit` (1..200), `offset` (>=0).

- Что и где ищется:
  - Без `smart`: SQL‑условие `ILIKE '%q%'` по полям `title`, `author`, `keywords`, `filename`, `text_excerpt` (+ `abstract` в v2).
  - Фильтр по тегам:
    - `tag=author=...` — ищется по `files.author`.
    - Прочие ключи — через `JOIN tags` и `tags.value ILIKE`.
  - Сортировка: по `mtime DESC NULLS LAST` (свежие сначала). В v2 возвращаются `items` и `total`.

- Умный режим (`smart`):
  - Лемматизация RU‑слов и небольшой словарь синонимов: `app.py:80..160` (`razdel`, `pymorphy2`, `_RU_SYNONYMS`).
  - Леммы исходного текста строки файла (`title/author/keywords/filename/text_excerpt/abstract` + `tags`) собираются функцией `_row_text_for_search`: `app.py:160`.
  - Алгоритм: сначала формируется базовая выборка с учётом всех фильтров, затем в Python отбрасываются элементы, у которых нет пересечения по леммам/синонимам (до 10 000 кандидатов, `mtime DESC`). Пагинация применяется уже к отфильтрованному списку.


## ИИ‑поиск (AI Search)
- API:
  - POST `/api/ai-search` — синхронный ответ с данными и массивом строк прогресса.
- POST `/api/ai-search/stream` — потоковый SSE‑ответ с событиями `progress` и финальным `result`.
  - Код: `app.py:6609`, `app.py:6648`, форматтер SSE: `app.py:6127`.
- GET `/app/admin/ai-metrics` — UI‑страница для просмотра/очистки журналов метрик (для администраторов).

- Входные параметры (JSON):
  - `query` — строка запроса (обязательная).
  - `top_k` — желаемое число результатов (кап: 1..5; сервер ограничивает до 5).
  - `deep_search` — флаг глубокого поиска по содержимому файлов.
  - `sources` — объект вида `{ tags: bool, text: bool }` для управления источниками кандидатов.
  - `max_candidates` — предел для списка кандидатов перед контентным анализом (по умолчанию 60).
  - `max_snippets`, `chunk_chars`, `max_chunks` — управление глубиной чтения файла (сниппеты на файл, размер чанка в символах, максимум чанков; дефолты 3 / 5000 / 40).
  - `use_rag` — `true|false`, включает/выключает использование RAG-контекста (по умолчанию `false`). Если `false`, сервер сразу переходит к классическому режиму и помечает ответ `rag_notes`.
  - `full_text` — включить чтение оригинального файла даже без `deep_search` (по умолчанию `false`).
  - Доп. фильтры: `collection_id` или `collection_ids[]`, `material_types[]`, `year_from`, `year_to`, `tag_filters[]` (`"key=value"`).

- Конвейер поиска
  1) Расширение запроса (ключевые слова)
     - Функция: `_ai_expand_keywords`: `app.py:6349`.
     - Формирует 3–8 тегов через LLM (`call_lmstudio_keywords`, `app.py:2789`) c отдельным промптом для поисковых запросов: выводит контекстные фразы без служебных слов, а выражения в кавычках включаются дословно.
     - Фразы в кавычках становятся обязательными терминами: документы без таких совпадений исключаются после отбора кандидатов.
     - Результат кэшируется в памяти (TTL по умолчанию 20 минут, `AI_EXPAND_TTL_MIN`). При сбое — наивное разбиение `query` на токены.

  2) Токенизация и стоп‑слова
     - Функция: `_tokenize_query`: `app.py:6180+` (блок рядом ниже `STOP_WORDS`).
     - Нормализация: в нижний регистр, удаление знаков, фильтрация коротких токенов, чисел и RU/EN стоп‑слов. Дедупликация, максимум 16 токенов.

  3) Подсчёт IDF
     - Функция: `_idf_for_terms`: `app.py:6180..6420` (начало блока).
     - Для каждого термина считается документо‑частота по объединению полей `files` и значений `tags` с учётом коллекций пользователя и `collections.searchable`. Используется сглаживание и лог‑масштабирование: чем реже слово, тем выше вес `idf`.

  4) Отбор кандидатов (источники)
     - Источники управляются через `sources`:
       - `tags: true` — поиск по тегам (`JOIN tags`).
       - `text: true` — поиск по метаданным/фрагментам (`files` колонки).
     - Для каждого термина `w` строятся SQL‑кандидаты (до 4000 строк на источник/термин) с фильтрами: коллекции, типы, годы, `tag_filters`.
       - Теги: `tags.value ILIKE '%w%'` или `tags.key ILIKE '%w%'`. За каждое срабатывание прибавляется `AI_SCORE_TAG * idf(w)`.
     - Поля файла: `ILIKE` по `title/author/keywords/text_excerpt/abstract`. За каждое совпадение — свой вес:
        - `AI_SCORE_TITLE`, `AI_SCORE_AUTHOR`, `AI_SCORE_KEYWORDS`, `AI_SCORE_EXCERPT`, `AI_SCORE_ABSTRACT` (см. `app.py:181..189`).
    - Все срабатывания накапливаются: суммарный балл `score` по файлу и подробные «хиты» (что и где совпало). Параллельно фиксируются уникальные термины, попавшие в файл.
    - После сортировки кандидаты ограничиваются `max_candidates` (по умолчанию 60), что ускоряет дальнейшие этапы.

 5) Сниппеты и ранжирование
    - Для всех набранных `file_id` повторно подтягиваются объекты `File` (с учётом фильтров и `searchable`).
     - Собираются сниппеты:
       - Из кэша `text_excerpt`/`static/cache/text_excerpts` (ф‑ция `_read_cached_excerpt_for_file`: `app.py:5835`) и короткий фрагмент из `abstract`.
       - Из фрагментов подбираются «окна» вокруг терминов: `_collect_snippets`: `app.py:5850`.
     - Бусты к баллу:
       - `phrase_boost` — точное вхождение исходной фразы запроса в `title/keywords` (`AI_BOOST_PHRASE`).
       - `coverage_boost` — за покрытие несколькими разными терминами (`AI_BOOST_MULTI`).
       - `prox_boost` — если в одном сниппете встречаются >=2 терминов (`AI_BOOST_SNIPPET_COOCCUR`).
     - Итоговая сортировка: по суммарному `score` (с бустами), затем по свежести (`mtime`). Код формирования: `app.py:6400..6520`.

  6) Глубокий/полнотекстовый поиск по контенту (`deep_search` / `full_text`)
     - Для топ‑кандидатов (минимум `max(top_k*2, 5)`) читается реальное содержимое файла блоками и ищутся совпадения на уровне текста.
     - Если `full_text=true`, сканируются все оставшиеся кандидаты (`max_candidates`), даже если `deep_search=false` — используется тот же механизм `_deep_scan_file`.
     - Чтение текста: `_iter_document_chunks`: `app.py:5910` — поддержка `.txt/.md/.pdf (PyMuPDF)/.docx/.rtf/.epub`; для PDF берётся текст со страниц, для docx — параграфы; блоки по ~5000 символов, ограничение по числу блоков.
     - Сканер: `_deep_scan_file`: `app.py:6012` — собирает сниппеты по терминам, фиксирует источники (`full-text` и др.), считает дополнительный `boost` ~ 1.5 за сниппет + 0.4 за уникальный термин; расширяет `snippets/snippet_sources/matched_terms`. Результат даёт дополнительный вклад в `score`.
     - Ход и результат по каждому кандидату логируются в прогресс.

  7) Краткий ответ LLM (поиск‑композер)
    - Из первых результатов формируется промпт: нумерованные фрагменты `[n]` (до 10), запрос пользователя и системная инструкция — дать краткий фактический ответ на русском с ссылками `[n]` по месту факта.
    - Вызов: `call_lmstudio_compose`: `app.py:2512`. Результат — поле `answer` и соответствующие строки прогресса: `app.py:6520..6609`.
    - При наличии RAG-индекса `_prepare_rag_context` подбирает чанки и строит структурированный промпт (`build_system_prompt` / `build_user_prompt`), валидирует ответ (`validate_answer`), записывает сессию (`rag_sessions`).
    - Ответ API `/api/ai-search` дополняется полями: `rag_context` (массив секций `[doc_id, chunk_id, preview, translation_hint, scores]`), `rag_validation` (флаги проверки ссылок), `rag_notes` (список информационных/предупреждающих сообщений), `rag_fallback` (bool-флаг, если пришлось вернуться к классическому режиму), `rag_session_id` (запись для аудита).

  8) LLM‑реранжирование (опционально)
     - Если `AI_RERANK_LLM=true` (см. `/api/settings`, `runtime_settings.json`, `app.py:248` и `app.py:3491/3543`), топ‑15 пересортировываются через отдельную LLM‑инструкцию (вернуть JSON‑массив `id` по убыванию релевантности). Код: `app.py:6520..6609`.

- Возвращаемый формат (`/api/ai-search`):
  - `query` — исходный запрос.
  - `keywords` — список термов после расширения и токенизации.
  - `answer` — краткий ответ LLM (может быть пустым).
  - `items[]` — список результатов:
    - `file_id`, `rel_path`, `title`, `score`, `hits[]` (что совпало), `snippets[]`, `snippet_sources[]`.
  - `progress[]` — строки прогресса (хронология шагов поиска).


## Прогресс и «постепенная» выдача
- Серверный прогресс
  - Класс `_ProgressLogger` (`app.py:6080+`) на каждом шаге добавляет строку в общий список и, при использовании стрима, отправляет её через эмиттер в очередь событий.
  - Потоковый API `/api/ai-search/stream` запускает поиск в отдельном потоке и возвращает SSE‑события:
    - `data: {"type":"progress","line":"..."}` — по мере выполнения,
    - `data: {"type":"result", "payload": { ...полный ответ... }}` — по завершении,
    - `data: {"type":"error",...}` при ошибках,
    - `data: [DONE]` — завершение потока. Код: `app.py:6648..6717`.

- Фронтенд и фолбэк
  - UI сначала подключается к `/api/ai-search/stream` и показывает прогресс по мере поступления SSE‑событий.
    - Клиентский код: `frontend/src/pages/Catalogue.tsx:200..360` (`runAiSearchStream`).
  - Если поток недоступен (браузер без ReadableStream/SSE или сеть оборвалась), интерфейс автоматически откатывается на синхронный `/api/ai-search` и «проигрывает» сохранённые строки `progress[]` через `runAiSearchFallback`.

Итого: прогресс в UI теперь «живой», а фолбэк остался только для несовместимых окружений.


## Фильтры, флаги и отбор кандидатов
- Флаги источников (`sources`):
  - `tags`: включает/исключает отбор кандидатов по таблице `tags`.
  - `text`: включает/исключает отбор по полям файла (`title/author/keywords/text_excerpt/abstract`).
- `deep_search`: включает «полнотекстовый» этап — сканирование содержимого файлов блоками с поиском терминов и сбором новых сниппетов/бустов.
- `top_k`: сервер ограничивает до 5 (даже если запросили больше), чтобы держать пайплайн быстрым.
- `max_candidates`: после первичного ранжирования оставляет только N лучших (дефолт 60) — меньше кандидатов, быстрее глубокий анализ.
- `max_snippets`: максимум сниппетов на документ (дефолт 3, распространяется и на кэшированные отрывки, и на сканы).
- `chunk_chars`, `max_chunks`: настройки чтения файла при глубоком/полнотекстовом анализе (размер чанка в символах и количество чанков; по умолчанию 5000 и 40).
- `full_text`: принудительно читает сам файл даже если `deep_search=false` (по умолчанию выключено — используется фрагмент/кэш).
- Фильтры сужают пространство поиска на всех шагах (и при подсчёте IDF, и при отборе кандидатов): `collection_id(s)`, `material_types`, `year_{from,to}`, `tag_filters`.
  - Особый случай: `author` в `tag_filters` фильтруется по `files.author` (без `JOIN tags`).
- Коллекции: учитываются только те, что доступны пользователю и помечены `searchable=true`.


## «Полнотекстовый ИИ‑поиск»: что это значит здесь
- Это не векторный поиск и не FTS‑индекс. «Полнотекстовый» этап — это буквальный проход по извлечённому тексту документа (или по кэшу фрагментов), проверка наличия терминов/фразы и сбор сниппетов.
- Источники текста:
  - `text_excerpt`/кэш во `static/cache/text_excerpts` собирается при сканировании библиотеки.
  - Для глубокой проверки читается содержимое файлов с помощью профильных библиотек (PDF/docx/rtf/epub). OCR относится к этапу извлечения текста при сканировании, а не к самому ИИ‑поиску.


## Настройки и параметры
- Весовые коэффициенты (ENV): `AI_SCORE_TITLE/…/TAG`, `AI_BOOST_PHRASE`, `AI_BOOST_MULTI`, `AI_BOOST_SNIPPET_COOCCUR` — см. `app.py:181..189`.
- LLM‑реранжирование: флаг `AI_RERANK_LLM` (см. `/api/settings`, `runtime_settings.json:49`, `app.py:248`, `app.py:3491/3543`).
- Ключевые слова/композер: эндпоинты LLM берутся из конфигурации `/api/settings` (пулы по «назначениям»: `keywords`, `compose`, `rerank`).
- LLM‑эндпоинты: в `/admin/llm` можно указать провайдера (`openai`‑совместимый или `ollama`), базовый URL, модель и вес. Вес добавляет копии эндпоинта в круговую очередь: чем он больше, тем чаще этот сервер выбирается. Если один экземпляр отвечает кодами занятости (`429/503/…`), запрос автоматически перейдёт к следующему свободному эндпоинту.
- Кэш сниппетов: `AI_SNIPPET_CACHE_TTL_HOURS` (дефолт 24 ч). Таблица `ai_search_snippet_cache`.
- Фоновая очистка: `AI_SNIPPET_CACHE_SWEEP_INTERVAL_HOURS` (дефолт 24 ч).
- Переключатель «Искать по всем языкам» (панель каталога → Настройки ИИ): при активном флаге фронтенд передаёт на `/api/ai-search` список языков из тегов `lang`. Бэкенд расширяет ключевые слова, добавляя переводы и синонимы на каждом языке (см. `_ai_expand_keywords`, `_ai_expand_multilingual_terms`) и выводит краткий отчёт вида `en(+3), de(+2)` для прозрачности. Даже без флага запросы на русском автоматически дополняются английскими терминами (и наоборот), чтобы сшивать выдачу RU↔EN. Итоговый ответ ИИ и LLM‑сниппеты всегда формулируются на русском языке.


## Точки входа в код (для навигации)
- Классический поиск: `app.py:3239` (`/api/search`), `app.py:3313` (`/api/search_v2`).
- ИИ‑поиск ядро: `_ai_search_core` — блок начинается рядом с `app.py:6180..6660` (см. шаги и комментарии в коде).
- Расширение ключевых слов (LLM): `call_lmstudio_keywords` — `app.py:2547`.
- Ответ LLM: `call_lmstudio_compose` — `app.py:2512`.
- Сниппеты, чтение кэша и файлов: `app.py:5835` (кэш), `app.py:5850` (сниппеты), `app.py:5910` (чтение блоков), `app.py:6012` (глубокий проход).
- Потоковый API (SSE): `/api/ai-search/stream` — `app.py:6648`.
- Фронтенд (каталог/поиск): `frontend/src/pages/Catalogue.tsx:200..420`.


## Обратная связь и метрики
- POST `/api/ai-search/feedback` — сохраняет клики и оценки релевантности (таблица `ai_search_keyword_feedback`). Используется для фильтрации шумных терминов.
- POST `/api/admin/ai-search/feedback/train` — агрегирует фидбек в таблицу `ai_search_feedback_model` и синхронизирует веса для ранжирования (поддерживает `{"async": true}`).
- GET `/api/admin/ai-search/feedback/model` — быстрая проверка текущих весов (топ положительных/отрицательных документов).
- GET `/api/admin/ai-search/metrics` — последние измерения длительностей (`ai_search_metrics`). Параметр `limit` (1..500).
- Инструменты эксплуатации: см. `docs/ai_search_operations.md`.


## Производительность и заметки
- Классический поиск с `smart=false` — максимально быстрый (полностью на SQL). `smart=true` выполняет дополнительную фильтрацию в Python — может быть тяжелее на больших выборках.
- ИИ‑поиск: этапы IDF/кандидаты — SQL‑ограничение до нескольких тысяч записей на термин; «глубокий» этап читает содержимое файлов и заметно дороже, используйте по необходимости.


## RAG план (расширенный)

Статусы: `[x]` — выполнено, `[~]` — в прогрессе, `[ ]` — запланировано.

### Индекс и подготовка данных
- [ ] **Поток импорта**: при загрузке файла определять тип (pdf/docx/html/txt), нормализовать кодировку, сохранять исходный full-text в `documents_raw`, конвертировать в `documents_clean` (очищенный текст) и фиксировать время импорта.
- [ ] **Извлечение структурных метаданных**: парсить заголовки, авторов, темы, язык, теги коллекций; сохранять в `documents_meta` с флажком `is_ready_for_rag` и журналом ошибок, чтобы поверху строить фильтры.
- [x] **Очистка и нормализация**: приводить пробелы, маркеры списков, удалять служебные разделители, заменять спецсимволы; хранить версию нормализатора (например `v1.2`) для аудита.
- [x] **Дедупликация**: вычислять SHA256 исходного файла, CRC чанков и `minhash` по биграммам текста; при коллизиях объединять версии и хранить карту `document_variant_id`.
- [x] **Чанкование**: разбивать текст на смысловые блоки 400–800 токенов с overlap 80–120 токенов, учитывать структуру заголовков, сохранять в `document_chunks` поля `chunk_id`, `section_path`, `token_count`, `hash`.
- [x] **Резюме чанков**: для каждого куска генерировать краткий сниппет (до 280 символов) и ключевые термины (`keywords_top`), чтобы быстро отображать превью и ускорять фильтры.
- [x] **Мультиязычная поддержка**: определять язык чанка (fastText/CLD3), сохранять признак `lang_primary` и список дополнительных языков, чтобы поддерживать смешанные документы.
- [x] **Эмбеддинги**: выбрать модель (например `intfloat/multilingual-e5-large`), хранить в `chunk_embeddings` поля `embedding`, `model_version`, `dim`, `created_at`. Обновления запускать батчево через `flask rag-embed-chunks` или отдельный скрипт.
- [x] **Индекс хранения**: dense-поиск на базе `rag_chunk_embeddings` (CLI `flask rag-search`/API `VectorRetriever`) и контрольные суммы векторов; sparse-поиск оставлен на SQL/BM25.
- [x] **Версионность**: при повторной индексации сохранять `index_snapshot_id`, журналировать изменения (added/updated/deleted) для возможности отката и диффа между версиями.
- [x] **Диагностика**: отдельная таблица `rag_ingest_failures` с причиной (ошибка OCR, неподдерживаемый формат, пустой текст), датой и ответственным пайплайном; раз в сутки рассылать отчеты.

### Подбор релевантного контекста
- [x] **Комбинированный поиск**: `KeywordRetriever` (SQL LIKE по чанкам) + `VectorRetriever`; CLI `flask rag-context` показывает `score_sparse`/`score_dense`, фильтрацию по языкам и ограничение кандидатов.
- [x] **Агрегатор чанков**: `ContextSelector` нормализует веса, применяет penalty за повторный документ, формирует reason hints и сохраняет слова/превью для UI.
- [x] **Rerank**: `ContextSelector` поддерживает `rerank_fn` (CLI `--rerank-mode dense|sparse|combined`), сохраняет `reasoning_hint` и позволяет подключить LLM/cross-encoder на списке кандидатов.
- [x] **Контроль длины**: селектор учитывает токены (`token_count` / эвристика) и режет выдачу по лимиту (`--max-tokens`, по умолчанию 3500) без потери первого чанка.
- [x] **Семантические фильтры**: настраиваемые пороги (`--min-dense-score`, `--min-sparse-score`, `--min-combined-score`) и языковые фильтры; CLI позволяет явно задавать список языков.

### Генерация ответов
- [x] **Системный промпт**: `build_system_prompt` (`agregator/rag/prompt.py`) задаёт формат «Факты/Источники» с fallback «Источников не найдено»; CLI `flask rag-generate` выводит итоговый системный промпт.
- [x] **Контекст в промпте**: `build_user_prompt` формирует блоки `doc_id/chunk`, включает язык, сниппет, цитату и опциональные оценки; `rag-generate` собирает промпт по выбранным чанкам.
- [x] **Многоязычность**: `build_user_prompt` добавляет `Подсказка` для разноязычных отрывков (на основе `detect_language`), CLI `rag-generate` следит за тем, чтобы ответ оставался на языке запроса.
- [x] **Логирование**: CLI `rag-generate` (опция `--store`) пишет запись в `rag_sessions` с промптами, ответом, параметрами и валидацией; таблица добавлена в `models.py`.

### Пост-обработка и валидация
- [x] **Проверка ссылок**: `validate_answer` гарантирует наличие `[doc_id:chunk]`, фиксирует строки без ссылок и подсвечивает их в UI.
- [x] **Согласованность**: проверка, что ссылки присутствуют в контексте; чужие/лишние ссылки попадают в предупреждения.
- [x] **Fallback**: при отсутствии контекста/эмбеддингов возвращается классический режим, в прогрессе и API поле `rag_fallback=true`, `rag_notes[]` сообщает причину.
- [x] **Пустой ответ**: при неуспешной генерации возвращается шаблон `fallback_answer` («Источников не найдено»).


### UX и аналитика
- [x] **Интерфейс**: карточка «Ответ RAG» показывает источники, предупреждения, подсказки перевода, контекст с кликабельными ссылками и кнопку открытия документа (фронтенд `AiPanel.tsx`).
- [x] **Статус выполнения**: прогресс-бар и иконки этапов (кандидаты → глубокий поиск → RAG → реранжирование → ответ) + предупреждение при fallback (`ProgressPanel`).
- [x] **Метрики**: `_record_search_metric` сохраняет длительности, `rag_context_count`, флаг fallback и `rag_hallucination_warning`; данные доступны через `/api/admin/ai-search/metrics`.

### Тестирование и эксплуатация
- [x] **Unit-тесты**: `tests/test_rag_indexing.py` и `tests/test_rag_prompt.py` покрывают нормализацию, чанкование, промпты, валидацию ссылок.
- [x] **Интеграционные тесты**: `tests/test_rag_pipeline.py` проходит путь «ингест → эмбеддинги → подбор контекста → генерация».
- [x] **Smoke-скрипт**: `scripts/rag_smoke.py` — цикл из типовых запросов, выводит времена и признаки fallback/предупреждений.
- [x] **Утилиты эксплуатации**: `scripts/rag_index.py` (подкоманды `rebuild/ingest/embed/inspect` для работы с индексом).
- CLI `scripts/rag_smoke.py --help` запускает smoke-тест; `scripts/rag_index.py rebuild` переиндексирует документы и пересчитывает эмбеддинги, `inspect` позволяет посмотреть конкретный chunk.
- [x] **Документация**: чек-лист мониторинга, утилиты `rag_index.py` / `rag_smoke.py` описаны в `docs/ai_search_operations.md`; обновлено описание ответа API.
