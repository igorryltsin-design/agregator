===== README.md =====
# Aggregator SPPR

Aggregator SPPR is a standalone early-fusion component for multimodal signals. The module registers data sources, normalizes weighted streams, builds relation graphs, and forms aligned selections for downstream analytics.

## Features
- Flask REST API for registering sources, ingesting signals, and launching fusion windows
- Reliability/intensity scoring with timestamp normalization
- Relation graph construction with provenance-aware edges and event history
- CLI for batch loading and diagnostics
- Built-in telemetry plus lightweight background scheduler

## Run
```bash
python -m program_modules.aggregator_sppp.cli runserver --host 0.0.0.0 --port 8051
```

CLI commands: `register-source`, `ingest-signal`, `fuse-window`, `stats`.


===== __init__.py =====
"""Aggregator SPPR module package."""

from .app import create_app
from .bootstrap import bootstrap_pipeline

__all__ = ["create_app", "bootstrap_pipeline"]


===== app.py =====
"""Flask application entry point for Aggregator SPPR."""

from __future__ import annotations

import atexit

from flask import Flask

from .api.routes import create_blueprint
from .bootstrap import bootstrap_pipeline


def create_app() -> Flask:
    ctx = bootstrap_pipeline()
    app = Flask(__name__)
    app.config["AGGREGATOR_CONFIG"] = ctx.config
    app.register_blueprint(create_blueprint(ctx.pipeline, ctx.telemetry), url_prefix="/api")
    atexit.register(ctx.scheduler.stop)
    atexit.register(ctx.telemetry.stop)
    return app


if __name__ == "__main__":
    app = create_app()
    app.run(host="0.0.0.0", port=8051)


===== bootstrap.py =====
"""Bootstrap helpers that assemble all runtime components."""

from __future__ import annotations

import logging
from pathlib import Path

from .config import AppConfig, load_config
from .database import Database
from .logging_setup import configure_logging
from .services.pipeline import AggregationPipeline
from .services.scheduler import SchedulerService
from .telemetry import Telemetry


class BootstrapContext:
    def __init__(
        self,
        config: AppConfig,
        database: Database,
        telemetry: Telemetry,
        pipeline: AggregationPipeline,
        scheduler: SchedulerService,
    ) -> None:
        self.config = config
        self.database = database
        self.telemetry = telemetry
        self.pipeline = pipeline
        self.scheduler = scheduler


def bootstrap_pipeline(base_dir: Path | None = None) -> BootstrapContext:
    configure_logging()
    config = load_config(base_dir)
    logging.getLogger("aggregator.bootstrap").info("Loaded config for %s", config.extra)
    database = Database(config.database_url)
    database.create_all()

    telemetry = Telemetry(config.telemetry)
    pipeline = AggregationPipeline(config, database, telemetry)
    scheduler = SchedulerService(config)
    scheduler.add_task(
        "telemetry_flush",
        interval=config.telemetry.export_interval.total_seconds(),
        handler=lambda: logging.getLogger("aggregator.telemetry").info(
            "Telemetry snapshot %s", telemetry.snapshot()
        ),
    )
    scheduler.start()
    telemetry.start()
    return BootstrapContext(config, database, telemetry, pipeline, scheduler)


===== cli.py =====
"""Command line interface for Aggregator SPPR."""

from __future__ import annotations

import argparse
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from .bootstrap import bootstrap_pipeline


def _shutdown(ctx):
    ctx.scheduler.stop()
    ctx.telemetry.stop()


def _print(data: Any) -> None:
    print(json.dumps(data, indent=2, ensure_ascii=False))


def cmd_runserver(args):
    from .app import create_app

    app = create_app()
    app.run(host=args.host, port=args.port)


def cmd_register_source(args):
    ctx = bootstrap_pipeline(Path(args.base_dir) if args.base_dir else None)
    result = ctx.pipeline.register_source(
        {
            "name": args.name,
            "modality": args.modality,
            "reliability": args.reliability,
            "priority": args.priority,
            "description": args.description,
        }
    )
    _print(result.payload if result.ok else {"error": result.error})
    _shutdown(ctx)


def cmd_ingest_signal(args):
    ctx = bootstrap_pipeline(Path(args.base_dir) if args.base_dir else None)
    payload = {
        "source_name": args.source,
        "signal_type": args.signal_type,
        "payload_ref": args.payload_ref,
        "recorded_at": datetime.utcnow().isoformat(),
        "metadata": {},
        "intensity": args.intensity,
    }
    result = ctx.pipeline.ingest_signal(payload)
    _print(result.payload if result.ok else {"error": result.error})
    _shutdown(ctx)


def cmd_fuse(args):
    ctx = bootstrap_pipeline(Path(args.base_dir) if args.base_dir else None)
    payload = {
        "window_start": (datetime.utcnow() - timedelta(minutes=args.window)).isoformat(),
        "window_end": datetime.utcnow().isoformat(),
    }
    result = ctx.pipeline.build_fusion(payload)
    _print(result.payload if result.ok else {"error": result.error})
    _shutdown(ctx)


def cmd_stats(args):
    ctx = bootstrap_pipeline(Path(args.base_dir) if args.base_dir else None)
    result = ctx.pipeline.stats()
    _print(result.payload)
    _shutdown(ctx)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Aggregator SPPR CLI")
    sub = parser.add_subparsers(dest="command", required=True)

    runserver = sub.add_parser("runserver", help="Start HTTP server")
    runserver.add_argument("--host", default="0.0.0.0")
    runserver.add_argument("--port", type=int, default=8051)
    runserver.set_defaults(func=cmd_runserver)

    reg = sub.add_parser("register-source", help="Create data source")
    reg.add_argument("--name", required=True)
    reg.add_argument("--modality", required=True)
    reg.add_argument("--reliability", type=float, default=0.5)
    reg.add_argument("--priority", type=int, default=5)
    reg.add_argument("--description")
    reg.add_argument("--base-dir")
    reg.set_defaults(func=cmd_register_source)

    ingest = sub.add_parser("ingest-signal", help="Ingest signal")
    ingest.add_argument("--source", required=True)
    ingest.add_argument("--signal-type", required=True)
    ingest.add_argument("--payload-ref", required=True)
    ingest.add_argument("--intensity", type=float, default=0.6)
    ingest.add_argument("--base-dir")
    ingest.set_defaults(func=cmd_ingest_signal)

    fuse = sub.add_parser("fuse-window", help="Fuse recent window")
    fuse.add_argument("--window", type=int, default=5)
    fuse.add_argument("--base-dir")
    fuse.set_defaults(func=cmd_fuse)

    stats = sub.add_parser("stats", help="Show stats")
    stats.add_argument("--base-dir")
    stats.set_defaults(func=cmd_stats)

    return parser


def main():
    parser = build_parser()
    args = parser.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()


===== config.py =====
"""Configuration helpers for the Aggregator SPPR module."""

from __future__ import annotations

import os
from dataclasses import dataclass, field
from datetime import timedelta
from pathlib import Path
from typing import Any, Dict


def _env(key: str, default: str) -> str:
    value = os.getenv(key, default)
    return value.strip() if isinstance(value, str) else default


def _env_int(key: str, default: int) -> int:
    try:
        return int(_env(key, str(default)))
    except ValueError:
        return default


def _env_float(key: str, default: float) -> float:
    try:
        return float(_env(key, str(default)))
    except ValueError:
        return default


def _env_bool(key: str, default: bool) -> bool:
    value = _env(key, str(default)).lower()
    if value in {"1", "true", "yes", "y"}:
        return True
    if value in {"0", "false", "no", "n"}:
        return False
    return default


@dataclass(slots=True)
class TelemetryConfig:
    enable_metrics: bool = True
    export_interval: timedelta = timedelta(seconds=30)
    window_size: int = 5000


@dataclass(slots=True)
class IngestionConfig:
    batch_size: int = 128
    default_confidence: float = 0.5
    default_intensity: float = 0.7
    deduplicate_window: timedelta = timedelta(seconds=15)


@dataclass(slots=True)
class FusionConfig:
    alignment_threshold: float = 0.65
    coherence_threshold: float = 0.5
    max_group_span: timedelta = timedelta(minutes=5)
    replay_window: timedelta = timedelta(hours=48)


@dataclass(slots=True)
class GraphConfig:
    similarity_threshold: float = 0.6
    max_neighbors: int = 20
    decay_factor: float = 0.92


@dataclass(slots=True)
class HistoryConfig:
    retention_days: int = 30
    auto_snapshot_interval: timedelta = timedelta(minutes=10)


@dataclass(slots=True)
class AppConfig:
    base_dir: Path
    database_url: str
    telemetry: TelemetryConfig = field(default_factory=TelemetryConfig)
    ingestion: IngestionConfig = field(default_factory=IngestionConfig)
    fusion: FusionConfig = field(default_factory=FusionConfig)
    graph: GraphConfig = field(default_factory=GraphConfig)
    history: HistoryConfig = field(default_factory=HistoryConfig)
    extra: Dict[str, Any] = field(default_factory=dict)

    @property
    def db_path(self) -> Path:
        if self.database_url.startswith("sqlite:///"):
            return Path(self.database_url[10:]).expanduser()
        return Path(self.base_dir / "aggregator_sppr.db")


def load_config(base_dir: Path | None = None) -> AppConfig:
    base_dir = base_dir or Path(os.getenv("AGGREGATOR_HOME", Path.cwd()))
    data_dir = base_dir / "data"
    data_dir.mkdir(parents=True, exist_ok=True)

    db_path = _env("AGGREGATOR_DB_PATH", str(data_dir / "aggregator_sppr.db"))
    telemetry = TelemetryConfig(
        enable_metrics=_env_bool("AGGREGATOR_METRICS", True),
        export_interval=timedelta(seconds=_env_int("AGGREGATOR_METRICS_INTERVAL", 30)),
        window_size=_env_int("AGGREGATOR_METRICS_WINDOW", 5000),
    )
    ingestion = IngestionConfig(
        batch_size=_env_int("AGGREGATOR_BATCH_SIZE", 128),
        default_confidence=_env_float("AGGREGATOR_DEFAULT_CONFIDENCE", 0.5),
        default_intensity=_env_float("AGGREGATOR_DEFAULT_INTENSITY", 0.7),
        deduplicate_window=timedelta(
            seconds=_env_int("AGGREGATOR_DEDUP_WINDOW", 15)
        ),
    )
    fusion = FusionConfig(
        alignment_threshold=_env_float("AGGREGATOR_ALIGN_THRESHOLD", 0.65),
        coherence_threshold=_env_float("AGGREGATOR_COHERENCE_THRESHOLD", 0.5),
        max_group_span=timedelta(
            minutes=_env_int("AGGREGATOR_MAX_GROUP_SPAN_MIN", 5)
        ),
        replay_window=timedelta(hours=_env_int("AGGREGATOR_REPLAY_WINDOW_H", 48)),
    )
    graph = GraphConfig(
        similarity_threshold=_env_float("AGGREGATOR_GRAPH_THRESHOLD", 0.6),
        max_neighbors=_env_int("AGGREGATOR_GRAPH_NEIGHBORS", 20),
        decay_factor=_env_float("AGGREGATOR_GRAPH_DECAY", 0.92),
    )
    history = HistoryConfig(
        retention_days=_env_int("AGGREGATOR_HISTORY_DAYS", 30),
        auto_snapshot_interval=timedelta(
            minutes=_env_int("AGGREGATOR_HISTORY_SNAPSHOT_MIN", 10)
        ),
    )
    extra = {
        "profile": _env("AGGREGATOR_PROFILE", "default"),
        "instance_id": _env("AGGREGATOR_INSTANCE_ID", "agg-sppr-local"),
    }

    return AppConfig(
        base_dir=base_dir,
        database_url=f"sqlite:///{db_path}",
        telemetry=telemetry,
        ingestion=ingestion,
        fusion=fusion,
        graph=graph,
        history=history,
        extra=extra,
    )


===== database.py =====
"""Database helpers for the Aggregator SPPR program."""

from __future__ import annotations

import contextlib
from typing import Iterator

from sqlalchemy import create_engine
from sqlalchemy.engine import Engine
from sqlalchemy.orm import DeclarativeBase, Session, sessionmaker


class Base(DeclarativeBase):
    """Declarative base for all ORM models."""


class Database:
    """Database wrapper that hides SQLAlchemy boilerplate."""

    def __init__(self, url: str, echo: bool = False) -> None:
        self._engine: Engine = create_engine(url, echo=echo, future=True)
        self._session_factory = sessionmaker(bind=self._engine, expire_on_commit=False)

    @property
    def engine(self) -> Engine:
        return self._engine

    def create_all(self) -> None:
        from . import models  # noqa: F401

        Base.metadata.create_all(self._engine)

    @contextlib.contextmanager
    def session(self) -> Iterator[Session]:
        session = self._session_factory()
        try:
            yield session
            session.commit()
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()


===== logging_setup.py =====
"""Custom logging configuration for the Aggregator SPPR module."""

from __future__ import annotations

import logging
import sys
from dataclasses import dataclass
from logging import Logger
from typing import Dict

DEFAULT_FORMAT = (
    "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
)


@dataclass(slots=True)
class LoggerConfig:
    level: int = logging.INFO
    fmt: str = DEFAULT_FORMAT
    enable_console: bool = True


def configure_logging(config: LoggerConfig | None = None) -> Dict[str, Logger]:
    config = config or LoggerConfig()
    logging.basicConfig(
        level=config.level,
        format=config.fmt,
        stream=sys.stdout,
    )
    logging.debug("Logging initialized with level %s", config.level)
    return {
        "aggregator": logging.getLogger("aggregator"),
        "pipeline": logging.getLogger("aggregator.pipeline"),
        "database": logging.getLogger("aggregator.database"),
        "telemetry": logging.getLogger("aggregator.telemetry"),
    }


===== models.py =====
"""ORM models for the Aggregator SPPR module."""

from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, List, Optional

from sqlalchemy import (
    JSON,
    Boolean,
    Column,
    DateTime,
    Enum,
    Float,
    ForeignKey,
    Integer,
    String,
    Text,
    UniqueConstraint,
)
from sqlalchemy.orm import Mapped, mapped_column, relationship

from .database import Base

Modality = Enum(
    "audio",
    "text",
    "visual",
    "radar",
    "composite",
    name="modality",
)


class DataSource(Base):
    __tablename__ = "data_sources"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(255), unique=True, nullable=False)
    modality: Mapped[str] = mapped_column(Modality, nullable=False)
    reliability_baseline: Mapped[float] = mapped_column(Float, default=0.5)
    priority: Mapped[int] = mapped_column(Integer, default=5)
    description: Mapped[str | None] = mapped_column(Text)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    signals: Mapped[List["Signal"]] = relationship("Signal", back_populates="source")


class Signal(Base):
    __tablename__ = "signals"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    source_id: Mapped[int] = mapped_column(ForeignKey("data_sources.id"))
    recorded_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    signal_type: Mapped[str] = mapped_column(String(64), nullable=False)
    payload_ref: Mapped[str] = mapped_column(String(512), nullable=False)
    vector: Mapped[List[float] | None] = mapped_column(JSON)
    attributes: Mapped[Dict[str, Any]] = mapped_column(JSON, default=dict)
    intensity: Mapped[float] = mapped_column(Float, default=0.0)
    reliability: Mapped[float] = mapped_column(Float, default=0.0)
    normalized: Mapped[bool] = mapped_column(Boolean, default=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    source: Mapped["DataSource"] = relationship("DataSource", back_populates="signals")
    fusion_links: Mapped[List["FusionLink"]] = relationship(
        "FusionLink", back_populates="signal", cascade="all, delete-orphan"
    )
    outgoing_edges: Mapped[List["RelationEdge"]] = relationship(
        "RelationEdge",
        foreign_keys="RelationEdge.from_signal_id",
        back_populates="from_signal",
    )
    incoming_edges: Mapped[List["RelationEdge"]] = relationship(
        "RelationEdge",
        foreign_keys="RelationEdge.to_signal_id",
        back_populates="to_signal",
    )


class FusionSample(Base):
    __tablename__ = "fusion_samples"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    start_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    end_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    summary: Mapped[str] = mapped_column(Text, nullable=False)
    context_score: Mapped[float] = mapped_column(Float, default=0.0)
    coherence: Mapped[float] = mapped_column(Float, default=0.0)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    links: Mapped[List["FusionLink"]] = relationship(
        "FusionLink", back_populates="sample", cascade="all, delete-orphan"
    )


class FusionLink(Base):
    __tablename__ = "fusion_links"
    __table_args__ = (
        UniqueConstraint("sample_id", "signal_id", name="uix_sample_signal"),
    )

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    sample_id: Mapped[int] = mapped_column(ForeignKey("fusion_samples.id"))
    signal_id: Mapped[int] = mapped_column(ForeignKey("signals.id"))
    weight: Mapped[float] = mapped_column(Float, default=0.0)

    sample: Mapped["FusionSample"] = relationship("FusionSample", back_populates="links")
    signal: Mapped["Signal"] = relationship("Signal", back_populates="fusion_links")


class RelationEdge(Base):
    __tablename__ = "relation_edges"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    from_signal_id: Mapped[int] = mapped_column(ForeignKey("signals.id"))
    to_signal_id: Mapped[int] = mapped_column(ForeignKey("signals.id"))
    relation_type: Mapped[str] = mapped_column(String(64), nullable=False)
    similarity: Mapped[float] = mapped_column(Float, default=0.0)
    weight: Mapped[float] = mapped_column(Float, default=0.0)
    window_start: Mapped[datetime] = mapped_column(DateTime)
    window_end: Mapped[datetime] = mapped_column(DateTime)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    from_signal: Mapped["Signal"] = relationship(
        "Signal",
        foreign_keys=[from_signal_id],
        back_populates="outgoing_edges",
    )
    to_signal: Mapped["Signal"] = relationship(
        "Signal",
        foreign_keys=[to_signal_id],
        back_populates="incoming_edges",
    )


class HistoryEvent(Base):
    __tablename__ = "history_events"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    event_type: Mapped[str] = mapped_column(String(64), nullable=False)
    payload: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    replay_token: Mapped[str] = mapped_column(String(64), nullable=False)


class ProcessingLog(Base):
    __tablename__ = "processing_logs"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    component: Mapped[str] = mapped_column(String(64), nullable=False)
    level: Mapped[str] = mapped_column(String(16), nullable=False)
    message: Mapped[str] = mapped_column(Text, nullable=False)
    extra: Mapped[Optional[Dict[str, Any]]] = mapped_column(JSON)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)


class ReplayCursor(Base):
    __tablename__ = "replay_cursors"
    __table_args__ = (
        UniqueConstraint("name", name="uix_replay_name"),
    )

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(128), nullable=False)
    last_event_id: Mapped[int] = mapped_column(Integer, default=0)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)


===== repositories.py =====
"""Repository layer that encapsulates persistence logic."""

from __future__ import annotations

from datetime import datetime, timedelta
from typing import Iterable, List, Sequence

from sqlalchemy import func, select
from sqlalchemy.orm import Session

from . import models


class BaseRepository:
    def __init__(self, session: Session):
        self.session = session


class DataSourceRepository(BaseRepository):
    def create(
        self,
        name: str,
        modality: str,
        reliability: float,
        priority: int,
        description: str | None = None,
    ) -> models.DataSource:
        entity = models.DataSource(
            name=name,
            modality=modality,
            reliability_baseline=reliability,
            priority=priority,
            description=description,
        )
        self.session.add(entity)
        self.session.flush()
        return entity

    def get_by_name(self, name: str) -> models.DataSource | None:
        return self.session.scalar(
            select(models.DataSource).where(models.DataSource.name == name)
        )

    def list_all(self) -> List[models.DataSource]:
        return list(self.session.scalars(select(models.DataSource)))


class SignalRepository(BaseRepository):
    def add(
        self,
        source_id: int,
        recorded_at: datetime,
        signal_type: str,
        payload_ref: str,
        vector: list[float],
        attributes: dict,
        intensity: float,
        reliability: float,
    ) -> models.Signal:
        entity = models.Signal(
            source_id=source_id,
            recorded_at=recorded_at,
            signal_type=signal_type,
            payload_ref=payload_ref,
            vector=vector,
            attributes=attributes,
            intensity=intensity,
            reliability=reliability,
            normalized=True,
        )
        self.session.add(entity)
        self.session.flush()
        return entity

    def fetch_window(
        self,
        window_start: datetime,
        window_end: datetime,
    ) -> List[models.Signal]:
        stmt = (
            select(models.Signal)
            .where(
                models.Signal.recorded_at >= window_start,
                models.Signal.recorded_at < window_end,
            )
            .order_by(models.Signal.recorded_at.asc())
        )
        return list(self.session.scalars(stmt))

    def latest_signals(self, limit: int = 50) -> List[models.Signal]:
        stmt = (
            select(models.Signal)
            .order_by(models.Signal.recorded_at.desc())
            .limit(limit)
        )
        return list(self.session.scalars(stmt))


class FusionRepository(BaseRepository):
    def create_sample(
        self,
        start_at: datetime,
        end_at: datetime,
        summary: str,
        context_score: float,
        coherence: float,
    ) -> models.FusionSample:
        entity = models.FusionSample(
            start_at=start_at,
            end_at=end_at,
            summary=summary,
            context_score=context_score,
            coherence=coherence,
        )
        self.session.add(entity)
        self.session.flush()
        return entity

    def attach_signals(
        self, sample_id: int, signal_ids: Sequence[int], weights: Sequence[float]
    ) -> None:
        for signal_id, weight in zip(signal_ids, weights, strict=False):
            link = models.FusionLink(
                sample_id=sample_id,
                signal_id=signal_id,
                weight=weight,
            )
            self.session.add(link)

    def latest_samples(self, limit: int = 20) -> List[models.FusionSample]:
        stmt = (
            select(models.FusionSample)
            .order_by(models.FusionSample.created_at.desc())
            .limit(limit)
        )
        return list(self.session.scalars(stmt))


class GraphRepository(BaseRepository):
    def upsert_relation(
        self,
        from_signal: int,
        to_signal: int,
        relation_type: str,
        similarity: float,
        weight: float,
        window_start: datetime,
        window_end: datetime,
    ) -> models.RelationEdge:
        stmt = select(models.RelationEdge).where(
            models.RelationEdge.from_signal_id == from_signal,
            models.RelationEdge.to_signal_id == to_signal,
            models.RelationEdge.relation_type == relation_type,
        )
        edge = self.session.scalar(stmt)
        if edge:
            edge.similarity = similarity
            edge.weight = weight
            edge.window_start = window_start
            edge.window_end = window_end
        else:
            edge = models.RelationEdge(
                from_signal_id=from_signal,
                to_signal_id=to_signal,
                relation_type=relation_type,
                similarity=similarity,
                weight=weight,
                window_start=window_start,
                window_end=window_end,
            )
            self.session.add(edge)
        self.session.flush()
        return edge

    def neighborhood(self, signal_id: int, limit: int = 20) -> List[models.RelationEdge]:
        stmt = (
            select(models.RelationEdge)
            .where(
                (models.RelationEdge.from_signal_id == signal_id)
                | (models.RelationEdge.to_signal_id == signal_id)
            )
            .order_by(models.RelationEdge.similarity.desc())
            .limit(limit)
        )
        return list(self.session.scalars(stmt))


class HistoryRepository(BaseRepository):
    def append(self, event_type: str, payload: dict, token: str) -> models.HistoryEvent:
        entity = models.HistoryEvent(
            event_type=event_type,
            payload=payload,
            replay_token=token,
        )
        self.session.add(entity)
        self.session.flush()
        return entity

    def fetch_window(self, since_id: int, limit: int = 100) -> List[models.HistoryEvent]:
        stmt = (
            select(models.HistoryEvent)
            .where(models.HistoryEvent.id > since_id)
            .order_by(models.HistoryEvent.id.asc())
            .limit(limit)
        )
        return list(self.session.scalars(stmt))

    def purge_older_than(self, cutoff: datetime) -> int:
        stmt = select(models.HistoryEvent.id).where(
            models.HistoryEvent.created_at < cutoff
        )
        ids = list(self.session.scalars(stmt))
        if not ids:
            return 0
        deleted = (
            self.session.query(models.HistoryEvent)
            .filter(models.HistoryEvent.id.in_(ids))
            .delete(synchronize_session=False)
        )
        return deleted


class LogRepository(BaseRepository):
    def add(self, component: str, level: str, message: str, extra: dict | None = None):
        entity = models.ProcessingLog(
            component=component,
            level=level,
            message=message,
            extra=extra,
        )
        self.session.add(entity)

    def latest(self, limit: int = 50) -> List[models.ProcessingLog]:
        stmt = (
            select(models.ProcessingLog)
            .order_by(models.ProcessingLog.created_at.desc())
            .limit(limit)
        )
        return list(self.session.scalars(stmt))


class ReplayCursorRepository(BaseRepository):
    def get(self, name: str) -> models.ReplayCursor | None:
        return self.session.scalar(
            select(models.ReplayCursor).where(models.ReplayCursor.name == name)
        )

    def update(self, name: str, event_id: int) -> models.ReplayCursor:
        cursor = self.get(name)
        if cursor:
            cursor.last_event_id = event_id
            cursor.updated_at = datetime.utcnow()
        else:
            cursor = models.ReplayCursor(name=name, last_event_id=event_id)
            self.session.add(cursor)
        self.session.flush()
        return cursor


def signal_activity_stats(session: Session, window: timedelta) -> dict:
    now = datetime.utcnow()
    since = now - window
    stmt = select(func.count(models.Signal.id)).where(
        models.Signal.created_at >= since
    )
    total = session.scalar(stmt) or 0
    by_modality = dict(
        session.execute(
            select(models.DataSource.modality, func.count(models.Signal.id))
            .join(models.Signal)
            .where(models.Signal.created_at >= since)
            .group_by(models.DataSource.modality)
        ).all()
    )
    return {"total": int(total), "by_modality": by_modality}


===== telemetry.py =====
"""Telemetry utilities for collecting runtime metrics."""

from __future__ import annotations

import threading
import time
from dataclasses import dataclass, field
from typing import Dict, List

from .config import TelemetryConfig


@dataclass
class MetricSample:
    timestamp: float
    value: float


@dataclass
class RollingMetric:
    name: str
    samples: List[MetricSample] = field(default_factory=list)
    max_samples: int = 5000

    def add(self, value: float) -> None:
        self.samples.append(MetricSample(time.time(), value))
        if len(self.samples) > self.max_samples:
            self.samples = self.samples[-self.max_samples :]

    def average(self) -> float:
        if not self.samples:
            return 0.0
        return sum(sample.value for sample in self.samples) / len(self.samples)

    def latest(self) -> float:
        return self.samples[-1].value if self.samples else 0.0


class Telemetry:
    """Lightweight metric collector with periodic export callback."""

    def __init__(self, config: TelemetryConfig):
        self.config = config
        self.metrics: Dict[str, RollingMetric] = {}
        self._lock = threading.Lock()
        self._stop_event = threading.Event()
        self._thread: threading.Thread | None = None
        self._export_callback = None

    def configure_export(self, callback) -> None:
        self._export_callback = callback

    def start(self) -> None:
        if not self.config.enable_metrics or self._thread:
            return

        def _loop():
            while not self._stop_event.wait(self.config.export_interval.total_seconds()):
                self._export()
            self._export()

        self._thread = threading.Thread(target=_loop, daemon=True)
        self._thread.start()

    def stop(self) -> None:
        if self._thread:
            self._stop_event.set()
            self._thread.join(timeout=2)
            self._thread = None

    def observe(self, name: str, value: float) -> None:
        if not self.config.enable_metrics:
            return
        with self._lock:
            metric = self.metrics.setdefault(
                name, RollingMetric(name=name, max_samples=self.config.window_size)
            )
            metric.add(value)

    def snapshot(self) -> Dict[str, dict]:
        with self._lock:
            return {
                name: {
                    "average": metric.average(),
                    "latest": metric.latest(),
                    "sample_count": len(metric.samples),
                }
                for name, metric in self.metrics.items()
            }

    def _export(self) -> None:
        if self._export_callback:
            try:
                self._export_callback(self.snapshot())
            except Exception:  # pragma: no cover
                pass


===== utils.py =====
"""Utility helpers for ingestion, normalization and scoring."""

from __future__ import annotations

import hashlib
import math
import random
import statistics
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Iterable, List, Sequence


def utc_now() -> datetime:
    return datetime.now(tz=timezone.utc).replace(tzinfo=None)


def normalize_timestamp(ts: datetime | str) -> datetime:
    if isinstance(ts, datetime):
        if ts.tzinfo:
            return ts.astimezone(timezone.utc).replace(tzinfo=None)
        return ts
    return datetime.fromisoformat(ts.replace("Z", "+00:00")).astimezone(
        timezone.utc
    ).replace(tzinfo=None)


def hash_payload(payload_ref: str) -> str:
    return hashlib.sha256(payload_ref.encode("utf-8")).hexdigest()


def random_vector(dim: int = 32) -> List[float]:
    return [round(random.uniform(-1, 1), 6) for _ in range(dim)]


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b, strict=False))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(y * y for y in b))
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)


def softmax(scores: Sequence[float]) -> List[float]:
    if not scores:
        return []
    exp_scores = [math.exp(s) for s in scores]
    total = sum(exp_scores)
    if total == 0:
        return [0.0 for _ in scores]
    return [s / total for s in exp_scores]


def clamp(value: float, min_value: float = 0.0, max_value: float = 1.0) -> float:
    return max(min_value, min(max_value, value))


def moving_average(values: Iterable[float], window: int = 5) -> List[float]:
    buffer: List[float] = []
    result: List[float] = []
    for value in values:
        buffer.append(value)
        if len(buffer) > window:
            buffer.pop(0)
        result.append(sum(buffer) / len(buffer))
    return result


def summarize_signals(texts: Sequence[str]) -> str:
    chunks = [text for text in texts if text]
    if not chunks:
        return "No signals available."
    if len(chunks) == 1:
        return chunks[0][:512]
    head = " | ".join(chunks[:5])
    return head[:1024]


def reliability_score(
    base: float,
    intensity: float,
    recency_sec: float,
    modality_bias: float,
) -> float:
    decay = math.exp(-recency_sec / 3600)
    score = base * 0.4 + intensity * 0.3 + modality_bias * 0.2 + decay * 0.1
    return clamp(score)


MODALITY_BIAS = {
    "audio": 0.5,
    "text": 0.6,
    "visual": 0.55,
    "radar": 0.7,
    "composite": 0.65,
}


def modality_bias(modality: str) -> float:
    return MODALITY_BIAS.get(modality, 0.5)


@dataclass(slots=True)
class WindowStats:
    intensity_avg: float
    reliability_avg: float
    spread_seconds: float


def window_stats(signals: Sequence[dict]) -> WindowStats:
    if not signals:
        return WindowStats(0.0, 0.0, 0.0)
    intensities = [item["intensity"] for item in signals]
    reliabilities = [item["reliability"] for item in signals]
    times = [item["timestamp"].timestamp() for item in signals]
    return WindowStats(
        intensity_avg=float(statistics.mean(intensities)),
        reliability_avg=float(statistics.mean(reliabilities)),
        spread_seconds=max(times) - min(times) if len(times) > 1 else 0.0,
    )


def expand_payload_ref(ref: str) -> str:
    if ref.startswith("file://"):
        return ref
    return f"file://{ref}"


===== api/routes.py =====
"""Flask blueprint exposing Aggregator SPPR API."""

from __future__ import annotations

from flask import Blueprint, jsonify, request

from ..services.pipeline import AggregationPipeline
from . import schemas


def create_blueprint(pipeline: AggregationPipeline, telemetry) -> Blueprint:
    bp = Blueprint("aggregator_api", __name__)

    @bp.route("/health", methods=["GET"])
    def health():
        return jsonify(schemas.success({"status": "ok"}).to_dict())

    @bp.route("/sources", methods=["POST"])
    def create_source():
        payload = request.get_json(force=True, silent=True) or {}
        result = pipeline.register_source(payload)
        envelope = (
            schemas.success(result.payload)
            if result.ok
            else schemas.failure(result.error or "error")
        )
        return jsonify(envelope.to_dict()), (200 if result.ok else 400)

    @bp.route("/sources", methods=["GET"])
    def list_sources():
        result = pipeline.list_sources()
        return jsonify(schemas.success(result.payload).to_dict())

    @bp.route("/signals", methods=["POST"])
    def ingest_signal():
        payload = request.get_json(force=True, silent=True) or {}
        result = pipeline.ingest_signal(payload)
        envelope = (
            schemas.success(result.payload)
            if result.ok
            else schemas.failure(result.error or "error")
        )
        status = 200 if result.ok else 400
        return jsonify(envelope.to_dict()), status

    @bp.route("/signals/latest", methods=["GET"])
    def latest_signals():
        limit = int(request.args.get("limit", 50))
        result = pipeline.latest_signals(limit=limit)
        return jsonify(schemas.success(result.payload).to_dict())

    @bp.route("/fusion", methods=["POST"])
    def fuse_window():
        payload = request.get_json(force=True, silent=True) or {}
        result = pipeline.build_fusion(payload)
        envelope = (
            schemas.success(result.payload)
            if result.ok
            else schemas.failure(result.error or "error")
        )
        status = 200 if result.ok else 400
        return jsonify(envelope.to_dict()), status

    @bp.route("/fusion/latest", methods=["GET"])
    def latest_samples():
        result = pipeline.latest_samples(limit=int(request.args.get("limit", 20)))
        return jsonify(schemas.success(result.payload).to_dict())

    @bp.route("/history", methods=["GET"])
    def history_window():
        cursor = request.args.get("cursor", "api")
        result = pipeline.history_window(cursor, limit=int(request.args.get("limit", 50)))
        return jsonify(schemas.success(result.payload).to_dict())

    @bp.route("/stats", methods=["GET"])
    def stats():
        result = pipeline.stats()
        snapshot = telemetry.snapshot()
        payload = result.payload | {"telemetry": snapshot}
        return jsonify(schemas.success(payload).to_dict())

    return bp


===== api/schemas.py =====
"""Response schema helpers for Flask views."""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict


def iso(dt: datetime | None) -> str | None:
    if not dt:
        return None
    return dt.replace(microsecond=0).isoformat() + "Z"


@dataclass
class Envelope:
    ok: bool
    payload: Dict[str, Any]
    error: str | None = None
    timestamp: str | None = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "ok": self.ok,
            "payload": self.payload,
            "error": self.error,
            "timestamp": self.timestamp or iso(datetime.utcnow()),
        }


def success(payload: Dict[str, Any]) -> Envelope:
    return Envelope(ok=True, payload=payload)


def failure(message: str) -> Envelope:
    return Envelope(ok=False, payload={}, error=message)


===== services/fusion.py =====
"""FusionEngine builds coherent samples from normalized signals."""

from __future__ import annotations

import logging
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Sequence, Tuple

from sqlalchemy.orm import Session

from ..config import AppConfig
from ..repositories import FusionRepository, SignalRepository
from .. import utils
from .validators import FusionWindowPayload

LOGGER = logging.getLogger("aggregator.fusion")


class FusionEngine:
    def __init__(self, config: AppConfig, telemetry) -> None:
        self.config = config
        self.telemetry = telemetry

    def fuse_window(self, session: Session, payload: FusionWindowPayload) -> dict:
        repo = SignalRepository(session)
        fusion_repo = FusionRepository(session)

        signals = repo.fetch_window(payload.window_start, payload.window_end)
        if not signals:
            return {"samples": []}

        grouped = self._group_signals(signals)
        samples = []
        for group in grouped:
            summary = utils.summarize_signals(
                [signal.attributes.get("text_summary") or signal.payload_ref for signal in group]
            )
            weights = utils.softmax([signal.reliability for signal in group])
            context_score = sum(
                signal.intensity * weight for signal, weight in zip(group, weights, strict=False)
            )
            coherence = self._estimate_coherence(group)

            sample = fusion_repo.create_sample(
                start_at=group[0].recorded_at,
                end_at=group[-1].recorded_at,
                summary=summary,
                context_score=context_score,
                coherence=coherence,
            )
            fusion_repo.attach_signals(
                sample.id,
                [signal.id for signal in group],
                weights,
            )
            samples.append(
                {
                    "sample_id": sample.id,
                    "signal_ids": [signal.id for signal in group],
                    "context_score": context_score,
                    "coherence": coherence,
                }
            )
            LOGGER.info(
                "Fusion sample %s created with %s signals", sample.id, len(group)
            )
            self.telemetry.observe("fusion.samples", 1.0)
        return {"samples": samples}

    def _group_signals(self, signals: Sequence) -> List[List]:
        if not signals:
            return []
        window_map: Dict[Tuple[str, int], List] = defaultdict(list)
        threshold = self.config.fusion.alignment_threshold
        span_limit = self.config.fusion.max_group_span
        for signal in signals:
            bucket_key = signal.source.modality
            assigned = False
            for key, bucket in window_map.items():
                head = bucket[0]
                time_delta = abs(
                    (signal.recorded_at - head.recorded_at).total_seconds()
                )
                similarity = utils.cosine_similarity(
                    signal.vector or [], head.vector or []
                )
                if (
                    time_delta <= span_limit.total_seconds()
                    and similarity >= threshold
                    and key[0] == signal.source.modality
                ):
                    bucket.append(signal)
                    assigned = True
                    break
            if not assigned:
                window_map[(bucket_key, signal.id)].append(signal)
        return [sorted(bucket, key=lambda s: s.recorded_at) for bucket in window_map.values()]

    def _estimate_coherence(self, signals: Sequence) -> float:
        if len(signals) < 2:
            return 1.0
        vectors = [signal.vector or [] for signal in signals]
        similarities = []
        for idx in range(len(vectors) - 1):
            similarities.append(
                utils.cosine_similarity(vectors[idx], vectors[idx + 1])
            )
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0
        intensities = [signal.intensity for signal in signals]
        intensity_spread = max(intensities) - min(intensities)
        coherence = avg_similarity - 0.1 * intensity_spread
        return utils.clamp(coherence, 0.0, 1.0)


===== services/graph.py =====
"""Graph builder that links signals by similarity and shared context."""

from __future__ import annotations

import itertools
import logging
from datetime import datetime
from typing import Iterable, List

from sqlalchemy.orm import Session

from ..config import AppConfig
from ..repositories import GraphRepository
from .. import utils

LOGGER = logging.getLogger("aggregator.graph")


class GraphBuilder:
    def __init__(self, config: AppConfig, telemetry) -> None:
        self.config = config
        self.telemetry = telemetry

    def update_graph(self, session: Session, signals: Iterable) -> List[int]:
        repo = GraphRepository(session)
        created_edges: List[int] = []
        signals = list(signals)
        for a, b in itertools.combinations(signals, 2):
            similarity = utils.cosine_similarity(a.vector or [], b.vector or [])
            if similarity < self.config.graph.similarity_threshold:
                continue
            relation_type = self._relation_type(a, b)
            weight = similarity * self.config.graph.decay_factor
            edge = repo.upsert_relation(
                from_signal=a.id,
                to_signal=b.id,
                relation_type=relation_type,
                similarity=similarity,
                weight=weight,
                window_start=min(a.recorded_at, b.recorded_at),
                window_end=max(a.recorded_at, b.recorded_at),
            )
            created_edges.append(edge.id)
        if created_edges:
            LOGGER.info("Graph updated with %s edges", len(created_edges))
            self.telemetry.observe("graph.edges", float(len(created_edges)))
        return created_edges

    def _relation_type(self, a, b) -> str:
        if a.source.modality == b.source.modality:
            return f"{a.source.modality}_coherence"
        if a.recorded_at.date() == b.recorded_at.date():
            return "temporal_proximity"
        return "context_bridge"


===== services/history.py =====
"""History service keeps sequential log of pipeline events."""

from __future__ import annotations

from datetime import datetime, timedelta
from typing import List

from sqlalchemy.orm import Session

from ..config import AppConfig
from ..repositories import HistoryRepository, ReplayCursorRepository


class HistoryService:
    def __init__(self, config: AppConfig):
        self.config = config

    def append(self, session: Session, event_type: str, payload: dict) -> int:
        repo = HistoryRepository(session)
        event = repo.append(event_type, payload, token=event_type[:8])
        return event.id

    def replay(self, session: Session, cursor_name: str, limit: int = 50) -> List[dict]:
        cursor_repo = ReplayCursorRepository(session)
        history_repo = HistoryRepository(session)

        cursor = cursor_repo.get(cursor_name)
        since_id = cursor.last_event_id if cursor else 0
        events = history_repo.fetch_window(since_id, limit=limit)
        if events:
            cursor_repo.update(cursor_name, events[-1].id)
        return [
            {
                "id": event.id,
                "event_type": event.event_type,
                "payload": event.payload,
                "created_at": event.created_at.isoformat(),
            }
            for event in events
        ]

    def purge(self, session: Session) -> int:
        repo = HistoryRepository(session)
        cutoff = datetime.utcnow() - timedelta(days=self.config.history.retention_days)
        return repo.purge_older_than(cutoff)


===== services/ingestion.py =====
"""Ingestion service: registers sources and normalizes signals."""

from __future__ import annotations

import logging
from collections import deque
from datetime import datetime
from typing import Deque, Tuple

from sqlalchemy.orm import Session

from ..config import AppConfig
from ..repositories import DataSourceRepository, LogRepository, SignalRepository
from .. import utils
from .validators import SignalPayload, SourcePayload, ValidationError

LOGGER = logging.getLogger("aggregator.ingestion")


class StreamIngestor:
    """Handles incoming signals, deduplication and normalization."""

    def __init__(self, config: AppConfig, telemetry) -> None:
        self.config = config
        self.telemetry = telemetry
        self._recent_signatures: Deque[Tuple[str, datetime]] = deque()

    def register_source(
        self, session: Session, payload: SourcePayload
    ) -> dict:
        repo = DataSourceRepository(session)
        existing = repo.get_by_name(payload.name)
        if existing:
            LOGGER.debug("Source %s already exists", payload.name)
            return {"id": existing.id, "created": False}

        entity = repo.create(
            name=payload.name,
            modality=payload.modality,
            reliability=payload.reliability,
            priority=payload.priority,
            description=payload.description,
        )
        LOGGER.info("Registered data source %s", payload.name)
        return {"id": entity.id, "created": True}

    def ingest_signal(self, session: Session, payload: SignalPayload) -> dict:
        ds_repo = DataSourceRepository(session)
        signal_repo = SignalRepository(session)
        log_repo = LogRepository(session)

        source = ds_repo.get_by_name(payload.source_name)
        if not source:
            raise ValidationError(f"Unknown source {payload.source_name}")

        signature = utils.hash_payload(payload.payload_ref)
        if self._is_duplicate(signature, payload.recorded_at):
            LOGGER.warning("Duplicate signal detected for %s", payload.payload_ref)
            log_repo.add(
                component="ingestion",
                level="warning",
                message="Duplicate signal rejected",
                extra={"payload_ref": payload.payload_ref},
            )
            return {"duplicate": True}

        vector = payload.metadata.get("vector") or utils.random_vector()
        recency_sec = (utils.utc_now() - payload.recorded_at).total_seconds()
        reliability = utils.reliability_score(
            base=source.reliability_baseline,
            intensity=payload.intensity,
            recency_sec=recency_sec,
            modality_bias=utils.modality_bias(source.modality),
        )

        entity = signal_repo.add(
            source_id=source.id,
            recorded_at=payload.recorded_at,
            signal_type=payload.signal_type,
            payload_ref=utils.expand_payload_ref(payload.payload_ref),
            vector=vector,
            attributes=payload.metadata,
            intensity=utils.clamp(payload.intensity),
            reliability=reliability,
        )

        log_repo.add(
            component="ingestion",
            level="info",
            message="Signal ingested",
            extra={
                "signal_id": entity.id,
                "source": payload.source_name,
                "reliability": reliability,
            },
        )
        self.telemetry.observe("signals.ingested", 1.0)
        return {
            "id": entity.id,
            "reliability": reliability,
            "intensity": entity.intensity,
        }

    def _is_duplicate(self, signature: str, recorded_at: datetime) -> bool:
        # purge old signatures
        window = self.config.ingestion.deduplicate_window
        while self._recent_signatures:
            sig, ts = self._recent_signatures[0]
            if (recorded_at - ts) > window:
                self._recent_signatures.popleft()
            else:
                break
        if any(sig == signature for sig, _ in self._recent_signatures):
            return True
        self._recent_signatures.append((signature, recorded_at))
        return False


===== services/pipeline.py =====
"""High level pipeline orchestrating ingestion, fusion and graph updates."""

from __future__ import annotations

import logging
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Dict

from .. import models
from ..config import AppConfig
from ..database import Database
from ..repositories import (
    DataSourceRepository,
    FusionRepository,
    LogRepository,
    SignalRepository,
)
from .fusion import FusionEngine
from .graph import GraphBuilder
from .history import HistoryService
from .ingestion import StreamIngestor
from .validators import FusionWindowPayload, SignalPayload, SourcePayload, ValidationError

LOGGER = logging.getLogger("aggregator.pipeline")


@dataclass
class PipelineResult:
    ok: bool
    payload: Dict[str, Any]
    error: str | None = None


class AggregationPipeline:
    def __init__(self, config: AppConfig, database: Database, telemetry) -> None:
        self.config = config
        self.database = database
        self.telemetry = telemetry
        self.ingestion = StreamIngestor(config, telemetry)
        self.fusion = FusionEngine(config, telemetry)
        self.graph = GraphBuilder(config, telemetry)
        self.history = HistoryService(config)

    @contextmanager
    def _session(self):
        with self.database.session() as session:
            yield session

    def register_source(self, payload: dict) -> PipelineResult:
        try:
            source = SourcePayload.from_dict(payload)
        except ValidationError as exc:
            return PipelineResult(ok=False, payload={}, error=str(exc))
        with self._session() as session:
            result = self.ingestion.register_source(session, source)
            self.history.append(
                session,
                "source_registered",
                {"name": source.name, "created": result["created"]},
            )
            return PipelineResult(ok=True, payload=result)

    def ingest_signal(self, payload: dict) -> PipelineResult:
        try:
            signal = SignalPayload.from_dict(payload)
        except ValidationError as exc:
            return PipelineResult(ok=False, payload={}, error=str(exc))
        with self._session() as session:
            try:
                result = self.ingestion.ingest_signal(session, signal)
            except ValidationError as exc:
                return PipelineResult(ok=False, payload={}, error=str(exc))
            if not result.get("duplicate"):
                self.history.append(
                    session,
                    "signal_ingested",
                    {
                        "signal_id": result["id"],
                        "source_name": signal.source_name,
                    },
                )
            return PipelineResult(ok=True, payload=result)

    def build_fusion(self, payload: dict) -> PipelineResult:
        window = FusionWindowPayload.from_dict(payload)
        with self._session() as session:
            result = self.fusion.fuse_window(session, window)
            repo = SignalRepository(session)
            for sample in result["samples"]:
                signals = [
                    self._load_signal(repo, signal_id)
                    for signal_id in sample["signal_ids"]
                ]
                signals = [signal for signal in signals if signal]
                if signals:
                    self.graph.update_graph(session, signals)
            if result["samples"]:
                self.history.append(
                    session,
                    "fusion_completed",
                    {
                        "sample_ids": [sample["sample_id"] for sample in result["samples"]],
                        "count": len(result["samples"]),
                    },
                )
            return PipelineResult(ok=True, payload=result)

    def list_sources(self) -> PipelineResult:
        with self._session() as session:
            repo = DataSourceRepository(session)
            sources = repo.list_all()
            return PipelineResult(
                ok=True,
                payload={
                    "sources": [
                        {
                            "id": source.id,
                            "name": source.name,
                            "modality": source.modality,
                            "reliability_baseline": source.reliability_baseline,
                            "priority": source.priority,
                        }
                        for source in sources
                    ]
                },
            )

    def latest_signals(self, limit: int = 50) -> PipelineResult:
        with self._session() as session:
            repo = SignalRepository(session)
            items = repo.latest_signals(limit=limit)
            return PipelineResult(
                ok=True,
                payload={
                    "signals": [
                        {
                            "id": signal.id,
                            "source": signal.source.name,
                            "modality": signal.source.modality,
                            "intensity": signal.intensity,
                            "reliability": signal.reliability,
                            "recorded_at": signal.recorded_at.isoformat(),
                        }
                        for signal in items
                    ]
                },
            )

    def latest_samples(self, limit: int = 20) -> PipelineResult:
        with self._session() as session:
            repo = FusionRepository(session)
            samples = repo.latest_samples(limit=limit)
            return PipelineResult(
                ok=True,
                payload={
                    "samples": [
                        {
                            "id": sample.id,
                            "summary": sample.summary,
                            "context_score": sample.context_score,
                            "coherence": sample.coherence,
                        }
                        for sample in samples
                    ]
                },
            )

    def history_window(self, cursor: str, limit: int = 50) -> PipelineResult:
        with self._session() as session:
            events = self.history.replay(session, cursor_name=cursor, limit=limit)
            return PipelineResult(ok=True, payload={"events": events})

    def stats(self) -> PipelineResult:
        with self._session() as session:
            signal_repo = SignalRepository(session)
            log_repo = LogRepository(session)
            signals = signal_repo.latest_signals(limit=10)
            logs = log_repo.latest(limit=10)
            return PipelineResult(
                ok=True,
                payload={
                    "recent_signals": [
                        {
                            "id": signal.id,
                            "source": signal.source.name,
                            "type": signal.signal_type,
                            "recorded_at": signal.recorded_at.isoformat(),
                        }
                        for signal in signals
                    ],
                    "logs": [
                        {
                            "component": log.component,
                            "level": log.level,
                            "message": log.message,
                            "created_at": log.created_at.isoformat(),
                        }
                        for log in logs
                    ],
                },
            )

    def _load_signal(self, repo: SignalRepository, signal_id: int):
        return repo.session.get(models.Signal, signal_id)


===== services/scheduler.py =====
"""Background scheduler that triggers periodic maintenance tasks."""

from __future__ import annotations

import logging
import threading
import time
from dataclasses import dataclass
from typing import Callable, Dict

from ..config import AppConfig

LOGGER = logging.getLogger("aggregator.scheduler")


@dataclass
class ScheduledTask:
    name: str
    interval: float
    handler: Callable[[], None]
    last_run: float = 0.0


class SchedulerService:
    def __init__(self, config: AppConfig):
        self.config = config
        self._tasks: Dict[str, ScheduledTask] = {}
        self._thread: threading.Thread | None = None
        self._stop = threading.Event()

    def add_task(self, name: str, interval: float, handler: Callable[[], None]) -> None:
        self._tasks[name] = ScheduledTask(name=name, interval=interval, handler=handler)

    def start(self) -> None:
        if self._thread:
            return

        def _loop():
            while not self._stop.is_set():
                now = time.time()
                for task in self._tasks.values():
                    if now - task.last_run >= task.interval:
                        try:
                            task.handler()
                            task.last_run = now
                        except Exception as exc:
                            LOGGER.exception("Scheduled task %s failed: %s", task.name, exc)
                self._stop.wait(1.0)

        self._thread = threading.Thread(target=_loop, daemon=True)
        self._thread.start()

    def stop(self) -> None:
        if self._thread:
            self._stop.set()
            self._thread.join(timeout=2)
            self._thread = None
            self._stop.clear()


===== services/validators.py =====
"""Input validators for ingestion and API payloads."""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Optional

from .. import utils


class ValidationError(Exception):
    pass


@dataclass(slots=True)
class SourcePayload:
    name: str
    modality: str
    reliability: float
    priority: int
    description: Optional[str]

    @classmethod
    def from_dict(cls, data: dict) -> "SourcePayload":
        required = {"name", "modality"}
        missing = required - data.keys()
        if missing:
            raise ValidationError(f"Missing fields: {', '.join(sorted(missing))}")
        return cls(
            name=data["name"],
            modality=data["modality"],
            reliability=float(data.get("reliability", 0.5)),
            priority=int(data.get("priority", 5)),
            description=data.get("description"),
        )


@dataclass(slots=True)
class SignalPayload:
    source_name: str
    signal_type: str
    payload_ref: str
    recorded_at: datetime
    metadata: dict
    intensity: float

    @classmethod
    def from_dict(cls, data: dict) -> "SignalPayload":
        required = {"source_name", "signal_type", "payload_ref", "recorded_at"}
        missing = required - data.keys()
        if missing:
            raise ValidationError(f"Missing fields: {', '.join(sorted(missing))}")
        return cls(
            source_name=data["source_name"],
            signal_type=data["signal_type"],
            payload_ref=data["payload_ref"],
            recorded_at=utils.normalize_timestamp(data["recorded_at"]),
            metadata=data.get("metadata", {}),
            intensity=float(data.get("intensity", 0.5)),
        )


@dataclass(slots=True)
class FusionWindowPayload:
    window_start: datetime
    window_end: datetime
    required_modalities: List[str]

    @classmethod
    def from_dict(cls, data: dict) -> "FusionWindowPayload":
        now = utils.utc_now()
        default_start = now - timedelta(minutes=5)
        window_start = utils.normalize_timestamp(
            data.get("window_start", default_start)
        )
        window_end = utils.normalize_timestamp(data.get("window_end", now))
        if window_end <= window_start:
            window_end = window_start + timedelta(minutes=1)
        return cls(
            window_start=window_start,
            window_end=window_end,
            required_modalities=list(data.get("required_modalities", [])),
        )

